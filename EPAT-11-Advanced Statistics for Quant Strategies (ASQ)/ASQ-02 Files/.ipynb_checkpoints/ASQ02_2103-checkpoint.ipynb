{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistics for Quantitative Trading\n",
    "<div class=\"alert alert-info\"><strong>Part II : Time Series Modeling with Python</strong></div>\n",
    "\n",
    "#### Notebook Created on: 6 July 2020\n",
    "##### Last Update: 24 Mar 2021\n",
    "##### Version 2.7\n",
    "##### Author: Vivek Krishnamoorthy\n",
    "\n",
    "## **Agenda for today**\n",
    "- Anatomy of a time series process\n",
    "- Modeling time series using decomposition\n",
    "    - Method I: `statsmodels` library\n",
    "    - Method II: `fbprophet` library\n",
    "- Testing for stationarity\n",
    "- Modeling time series using\n",
    "    - Method III: Exponential smoothing\n",
    "    - Method IV: ARIMA\n",
    "- A brief glance at modeling volatility using ARCH/GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'convert'></a>\n",
    "\n",
    "### Anatomy of a time series process\n",
    "\n",
    "Time series processes have a wide variety of patterns. So it is helpful to consider them as a combination of *systematic* and *unsystematic* components.\n",
    "\n",
    "- **Systematic**: These are recurring in nature and so can be described and modeled.\n",
    "- **Non-systematic**: These are random in nature and so cannot be directly modeled.\n",
    "\n",
    "The systematic components can be further split into *level*, *trend*, and *seasonality* whereas the non-systematic component is referred to as *noise*.\n",
    "\n",
    "- **Level**: The average value of the process.\n",
    "- **Trend**: The direction and rate of change of the process. The slope is a good proxy for it.\n",
    "- **Seasonality**: Deviations in the process caused by recurring short-term cycles.\n",
    "- **Noise**: The random variation observed in the process.\n",
    "\n",
    "Another useful abstraction while analyzing time series processes is to see them as either an *additive* or a *multiplicative* blend of the four constituent parts mentioned.\n",
    "\n",
    "**Additive model**: The process $X(t)$ has the form\n",
    "$$X(t) = Level + Trend + Seasonality + Noise$$\n",
    "\n",
    "We use an additive model when the underlying process under examination has the following characteristics.\n",
    "- The process changes remain constant over time (i.e. they are linear). So the trend line would be straight.\n",
    "- It shows linear seasonality. That is to say the frequency and amplitude (i.e. the width and the height) of the cycles remain constant over time.\n",
    "\n",
    "\n",
    "\n",
    "**Multiplicative model**: The process $X(t)$ has the form\n",
    "$$X(t) = Level \\times Trend \\times Seasonality \\times Noise$$\n",
    "\n",
    "We use a multiplicative model when the underlying process under examination has the following characteristics.\n",
    "- The process changes vary over time (i.e. they are non-linear in nature).\n",
    "- An exponential or quadratic or higher order polynomial process is multiplicative. So the trend-line would be curved, not straight. \n",
    "- It shows non-linear seasonality. That is to say the frequency and amplitude (i.e. the width and the height) of the cycles vary over time.\n",
    "\n",
    "In the (harsh) real world, we often encounter non-linear or even mixed processes and therefore have to work with the multiplicative model as our prototype. But we prefer to work with linear processes as they are easier to use. So we transform the original process into a linear one. A commonly used trick is applying a log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling a time series using decomposition\n",
    "\n",
    "There are several available libraries (ex. [`statsmodels`](https://www.statsmodels.org/stable/tsa.html), [`fbprophet`](https://facebook.github.io/prophet/docs/quick_start.html), [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), [`PyFlux`](https://pyflux.readthedocs.io/en/latest/), [`fecon236`](https://github.com/MathSci/fecon236), [`PM-Prophet`](https://github.com/luke14free/pm-prophet) at the time of this writing) in Python to develop sophisticated time series models for forecasting. \n",
    "\n",
    "We work with `statsmodels` and `fbprophet` here. Both offer convenient routines to automatically decompose a time series into their fundamental components.\n",
    "\n",
    "To illustrate the ideas, we make use of the daily historical prices of crude oil (from 2003 to 2020) and soybean (from 2000 to 2020).\n",
    "\n",
    "> *The OPEC Reference Basket (ORB), also referred to as the OPEC (Organization of Petroleum Exporting Countries ex. Qatar, Saudi Arabia, Iran, Iraq) Basket, is a weighted average of prices for petroleum blends produced by OPEC members. It is used as an important benchmark for crude oil prices. The OPEC Basket, including a mix of light and heavy crude oil products, is heavier than both Brent crude oil, and West Texas Intermediate crude oil.* - [Source](https://www.investopedia.com/terms/o/opecbasket.asp)\n",
    "\n",
    "> *The soya bean is a species of legume and is one of the world’s most important oil plants, since around half of vegetable oil produced is obtained from the soya bean. It is particularly significant because of its protein content (39 per cent) and its oil content (17 per cent), since no other plant offers comparable values. Soya is also used as an ingredient and additive in the food industry. It is estimated that around 30,000 foods contain ingredients derived from soya. Soya milk can be produced from ground yellow soya beans, and then can be processed into tofu. Tofu is used as a meat substitute in vegetarian cookery because of its high protein content and because it contains all the essential amino acids.* - [Source](https://markets.businessinsider.com/commodities/soybeans-price/usd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We first try the `seasonal_decompose` method from the `statsmodels.tsa` sub-library and then experiment with the `fbprophet` library.\n",
    "\n",
    "#### Method I : Seasonal decomposition using the `seasonal_decompose` routine in `statsmodels` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import quandl\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quandl_apikey file not found\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "################### TO STUDENTS: PLEASE IGNORE THIS CELL ####################\n",
    "#############################################################################\n",
    "\n",
    "## I do the below procedure so as to not show my API key.\n",
    "## You can choose to ignore it.\n",
    "\n",
    "filename = \"quandl_apikey\"\n",
    "\n",
    "def get_file_contents(filename):\n",
    "    \"\"\" If provided a filename,\n",
    "        this function returns the contents of that file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            # We create a file containing a single line,\n",
    "            # with our Quandl API key\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{filename} file not found\")\n",
    "\n",
    "quandl_key = get_file_contents(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c844bab77ed1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Garbled characters appear at the start of my API key, hence the below step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mquandl_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquandl_key\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m## if above doesn't work, try quandl_key[3:] instead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mquandl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mApiConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquandl_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "## Garbled characters appear at the start of my API key, hence the below step\n",
    "quandl_key = 'QyireicTms8E9zxHTHZT' #quandl_key[1:]\n",
    "## if above doesn't work, try quandl_key[3:] instead\n",
    "quandl.ApiConfig.api_key = quandl_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################### TO STUDENTS: PLEASE RUN THIS CELL #######################\n",
    "#############################################################################\n",
    "\n",
    "## If you have installed quandl and created a free account there, you would have an API key.\n",
    "## Please copy-paste below as shown and replace YOURAPIKEY with your key from quandl.\n",
    "## Then uncomment the below line and run the cell.\n",
    "# quandl.ApiConfig.api_key = \"YOURAPIKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(\"crude_oil_prices.csv\")\n",
    "# df4 = pd.read_csv(\"crude_oil_prices.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start4 = '2003-01-01'\n",
    "end4 = '2020-07-05'\n",
    "ticker4 = \"OPEC/ORB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***********************************************************\n",
    "## ***** EXPERIMENTAL : IGNORE THIS CELL *********************\n",
    "## Trials with other commodity data\n",
    "## ***********************************************************\n",
    "\n",
    "# \"OPEC/ORB\" this is crude oil prices per barrel\n",
    "# WGC/GOLD_DAILY_INR for daily gold prices in India available until 10 March 2020\n",
    "# TFGRAIN/SOYBEANS for daily soy bean prices per bushel\n",
    "# WORLDAL/PALPROD primary aluminium production across continents. not tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = quandl.get(dataset=ticker4, start_date=start4, end_date=end4)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker4} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4.head(10))\n",
    "print(df4.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.rename(columns={'Value': 'price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.plot(figsize=(12, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- There are upward and downward trends in the prices. Looks linear. Needs further probing.\n",
    "- There seems to be seasonality and we can investigate further by looking at some moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 21 # for 1 monthly moving average calculations\n",
    "window_length2 = 252 # for annual moving average calculations\n",
    "\n",
    "## Calculating 21-day rolling mean and volatility\n",
    "\n",
    "df4['rolling_21d_mean'] = df4['price'].rolling(window=window_length).mean()\n",
    "df4['rolling_21d_vol'] = df4['price'].rolling(window=window_length).std()\n",
    "\n",
    "\n",
    "## Calculating 252-day rolling mean and volatility\n",
    "\n",
    "df4['rolling_12m_mean'] = df4['price'].rolling(window=window_length2).mean()\n",
    "df4['rolling_12m_vol'] = df4['price'].rolling(window=window_length2).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.25)\n",
    "df4.plot(figsize=(12, 9))\n",
    "\n",
    "plt.title(\"OPEC benchmark crude oil prices over time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price per barrel (in US$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The yearly moving average of the prices show a linear trend (which changes roughly every couple of years).\n",
    "- The monthly moving price average shows seasonality.\n",
    "- The rolling volatility is time-varying in both (monthly and annual) cases.\n",
    "- Let's try using the **multiplicative** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method I: Using statsmodels\n",
    "\n",
    "decompose_series = seasonal_decompose(df4['price'], period=252, model=\"multiplicative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "\n",
    "decompose_series.observed.plot(ax=ax[0])\n",
    "ax[0].set_title(\"Time series of crude oil prices\", fontsize=16)\n",
    "ax[0].set(xlabel=\"\", ylabel=\"Oil price (in US$/barrel)\")\n",
    "\n",
    "decompose_series.trend.plot(ax=ax[1])\n",
    "ax[1].set(xlabel=\"\", ylabel=\"Trend\")\n",
    "\n",
    "decompose_series.seasonal.plot(ax=ax[2])\n",
    "ax[2].set(xlabel=\"\", ylabel=\"Seasonal\")\n",
    "\n",
    "decompose_series.resid.plot(ax=ax[3])\n",
    "ax[3].set(xlabel=\"Date\", ylabel=\"Residual\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_series.seasonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The plot shows us the observed, trend, seasonal and residual time series. We can access each component by typing: `decompose_series.trend`, `decompose_series.seasonal`, and `decompose_series.residual` \n",
    "- The trend and seasonal plots that have been extracted from the original series look plausible.\n",
    "- The residual plot clearly has non-constant volatility. If the model was a suitable fit, then after taking out the trend and seasonality present in the price data, we would have residuals that do not have any discernable pattern. Not so here.\n",
    "- At this stage, we would evaluate alternatives to model the residuals. We could even consider exogenous variables like oil production, renewable energy investments, etc. which would influence oil prices (outside the scope of this session).\n",
    "- From the `statsmodels` documentation: *This is a naive decomposition. More sophisticated methods should be preferred.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method II : Seasonal decomposition using Facebook's Prophet library `fbprophet`\n",
    "\n",
    "We use the daily soybean prices from 2000 through 2018 to fit the model and make forecasts for the period from January 2019 to Feb 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method II: Using fbprophet\n",
    "## Working with the soy bean price series\n",
    "\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start5 = '2000-12-01'\n",
    "end5 = '2020-02-29'\n",
    "ticker5 = \"TFGRAIN/SOYBEANS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = quandl.get(dataset=ticker5, start_date=start5, end_date=end5)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker5} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(\"soy_bean_prices.csv\")\n",
    "# df4 = pd.read_csv(\"soy_bean_prices.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[['Cash Price', 'Fall Price', 'Basis', 'Fall Basis']].plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[['Cash Price', 'Fall Price', 'Basis']].plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- We will only work with `Cash Price` for the forecasting. There are both increasing and decreasing trends in the cash prices.\n",
    "- Since soy beans are an agricultural commodity, we see seasonality through the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.drop(columns=['Basis', 'Fall Price', 'Fall Basis'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following the DataFrame manipulation procedure and further steps\n",
    "## as recommended in Facebook's Prophet documentation\n",
    "df5.reset_index(drop=False, inplace=True)\n",
    "df5.rename(columns={'Date': 'ds', 'Cash Price': 'y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We split the data into training and test sets (this is the language of machine learning)\n",
    "## In the language of econometrics, we would call them in-sample and out-of-sample\n",
    "## Here, we choose data before 2019 as part of the training data set\n",
    "\n",
    "train_index = df5['ds'].apply(lambda x: x.year) < 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the training set\n",
    "\n",
    "df5_train = df5.loc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for NaNs\n",
    "\n",
    "df5_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_train.head(), df5_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the test set\n",
    "\n",
    "df5_test = df5.loc[~train_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for NaNs\n",
    "\n",
    "df5_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_test.head(), df5_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape, df5_train.shape, df5_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Instantiating the Prophet model\n",
    "## By default, it is 'additive'.\n",
    "## On your own time, you can try 'multiplicative' while reviewing the material\n",
    "model1 = Prophet(seasonality_mode='additive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Additional specification that our data has monthly seasonality\n",
    "## Other arguments have been set as per the documentation\n",
    "model1.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "\n",
    "## The below method is similar to the scikit-learn library's fit().\n",
    "## The model is fitted using the training data specified earlier.\n",
    "model1.fit(df5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df5_future = model1.make_future_dataframe(periods=425)\n",
    "df5_pred = model1.predict(df5_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_future.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_future.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_future.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_pred.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the predicted values on test and training data\n",
    "\n",
    "model1.plot(df5_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The model's predicted values (blue line) approximately follow the observed soy bean prices (black dots).\n",
    "- The light blue shadow is the confidence interval for the predicted values. Its width changes over time and quantifies our confidence in the point estimates.\n",
    "- Visually we see that whenever there's a sharp change in prices, the model fails to predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.plot_components(df5_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The overall trend shows an increase in cash prices over the entire time period.\n",
    "- In the weekly plot, we ignore the prices on weekends (no trading). The prices through the week are quite constant (i.e. no day-of-the-week effect).\n",
    "- In the yearly plot, there's a wide range of prices (of ~US\\$ 1 with a high in July and a low in October) observed across the year. We would expect to see seasonality in an agricultural commodity like soy beans (month-of-the-year effect).\n",
    "- In the monthly plot, we see some variance but the scale is much smaller than in the previous plot (i.e. no day-of-the-month effect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## We create a merged DataFrame to examine the actuals v/s predicted values closely\n",
    "\n",
    "selected_columns = ['ds', 'yhat_lower', 'yhat_upper', 'yhat']\n",
    "df5_pred = df5_pred.loc[:, selected_columns].reset_index(drop=True)\n",
    "\n",
    "## Using left join, we only select rows that are part of our test data set.\n",
    "## This would exclude the predictions on holidays and weekends.\n",
    "\n",
    "df5_test = df5_test.merge(df5_pred, on=['ds'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_test['ds'] = pd.to_datetime(df5_test['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_test.set_index('ds', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax = sns.lineplot(data=df5_test[['y', 'yhat_lower', \n",
    "                                 'yhat_upper', 'yhat']])\n",
    "ax.fill_between(df5_test.index, df5_test.yhat_lower, \n",
    "                df5_test.yhat_upper, alpha=0.3)\n",
    "ax.set(title='Soy bean price - actual vs. predicted', \n",
    "       xlabel='Date', ylabel='Price per bushel (US$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The interval estimate of the soy bean price prediction appears to have been accurate for the whole period (except a month in May 2019).\n",
    "- The confidence interval in the last six months is wider than in the initial period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for stationarity\n",
    "\n",
    "There are three ways of checking for stationarity in a time series.\n",
    "1. Visual inspection\n",
    "2. Statistical tests\n",
    "3. ACF/PACF plots\n",
    "\n",
    "We prefer working with stationary time series processes because it makes modeling, analysis and forecasting more feasible.\n",
    "\n",
    "For this section, we work with the last 25 years of daily gold prices in India. The prices shown are denominated in INR per ounce.\n",
    "\n",
    "> *Gold is a liquid asset, ranking at levels comparable to many global stock markets as well as currency spreads. Its liquidity is often sourced during periods of stress in the markets, one of its appealing qualities.* - [Source](https://www.gold.org/goldhub/data/trading-volumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start6 = '1995-01-01'\n",
    "end6 = '2020-02-29'\n",
    "ticker6 = \"WGC/GOLD_DAILY_INR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = quandl.get(dataset=ticker6, start_date=start6, end_date=end6)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker6} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(\"gold_prices_inr.csv\")\n",
    "# df4 = pd.read_csv(\"gold_prices_inr.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.rename(columns={\"Value\": \"price\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats as sms\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(y, wl1=21, wl2=252, lags=40, figsize=(15, 10)):\n",
    "    \"\"\" Checks the stationarity of a pandas Series (default is daily prices or returns),\n",
    "        using plots, correlograms and the ADF test\n",
    "    \"\"\"\n",
    "    ## Calculating rolling statistics\n",
    "    \n",
    "    rolling_wl1_mean = y.rolling(window=wl1).mean()\n",
    "    rolling_wl2_mean = y.rolling(window=wl2).mean()\n",
    "    rolling_wl1_vol = y.rolling(window=wl1).std()\n",
    "    rolling_wl2_vol = y.rolling(window=wl2).std()\n",
    "    \n",
    "    ## Plotting the price, rolling statistics and correlograms\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    sns.set(font_scale=1)\n",
    "    layout = (2, 2)\n",
    "    y_ax = plt.subplot2grid(layout, (0, 0))\n",
    "    vol_ax = plt.subplot2grid(layout, (0, 1))\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        \n",
    "    y.plot(ax=y_ax)\n",
    "    rolling_wl1_mean.plot(ax=y_ax)\n",
    "    rolling_wl2_mean.plot(ax=y_ax)\n",
    "    \n",
    "    rolling_wl1_vol.plot(ax=vol_ax)\n",
    "    rolling_wl2_vol.plot(ax=vol_ax)\n",
    "    y_ax.set_title('Rolling means over time')\n",
    "    y_ax.legend(['observed', f'{wl1}-period MA of observed', f'{wl2}-period MA of observed'], loc='best')\n",
    "    #y_ax.set_ylabel(\"Gold prices(in INR)/oz.\")\n",
    "    \n",
    "    vol_ax.set_title('Rolling volatility over time')\n",
    "    vol_ax.legend([f'{wl1}-period MA of volatility', f'{wl2}-period MA of volatility'], loc='best')\n",
    "    \n",
    "    sm.graphics.tsa.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.05)\n",
    "    sm.graphics.tsa.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.05)\n",
    "    \n",
    "    ## Running the Augmented Dickey-Fuller test\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('--------- The augmented Dickey-Fuller test results -----------')\n",
    "    print('--------------------------------------------------------------')\n",
    "    adftest = adfuller(y, autolag='AIC')\n",
    "    results = pd.Series(adftest[0:4], index=['Test Statistic','p-value','# of Lags','# of Observations'])\n",
    "    for key,value in adftest[4].items():\n",
    "        results[f'Critical Value ({key})'] = '{0:.3f}'.format(value)\n",
    "    print(results)\n",
    "    print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df6['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- In the ADF test, if the test statistic is greater than the critical value, we conclude that the series is non-stationary. We can draw the same conclusion by examining the p-value. A p-value greater than our significance level (conventionally 5%) means we cannot reject our null hypothesis (The series is not stationary). \n",
    "- For the gold prices, we have a p-value of nearly 1 (and equivalently the test statistic is greater than the critical values at all 3 significance levels), so we conclude that the price series is not stationary.\n",
    "- The rolling means and volatility plots are time-varying. So we also conclude visually that gold prices in India are non-stationary.\n",
    "- From the ACF, there are significant autocorrelations above the 95% confidence interval at all lags. From the PACF, we have significance in autocorrelations at lags 1, 2, 3, 6, and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6['log_returns'] = np.log(df6['price'] / df6['price'].shift(1))\n",
    "df6.dropna(axis='rows', how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df6['log_returns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- As per the ADF test results, the returns of gold are stationary since the p-value is almost 0 and the test statistic is less than all the critical values.\n",
    "- The returns and rolling means of the returns are all centred around 0. As the time scale increases, the means become more and more constant. At shorter time scales, the noise tends to obscure the signal.\n",
    "- The volatily is time-varying at both the faster and slower rolling levels.\n",
    "- There are little spikes in the ACF plot at lags 3, 11, and 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method III : Modeling time series using exponential smoothing\n",
    "\n",
    "This method works well with non-stationary data. It is similar to exponential moving averages in that higher weights are assigned to more recent data. It is an alternative to the ARIMA class of methods (which we will explore later).\n",
    "\n",
    "In exponential smoothing, the forecasts are a weighted sum of past observations wherein the weights decrease exponentially as we move further into the past.\n",
    "\n",
    "##### 1. Simple Exponential Smoothing\n",
    "\n",
    "- Most suited for a series that does not exhibit any clear trend or seasonality.\n",
    "- We use a level-smoothing factor, $\\alpha$, to set the rate at which the weight assigned to past observations decay. It takes a value between $0$ and $1$. In `statsmodels`, its called `smoothing_level`.\n",
    "- The higher the value of $\\alpha$, the more weight is assigned to observations in the recent past. The lower the value of $\\alpha$, the more weight is assigned to observations in the distant past. \n",
    "- In the degenerate case of $\\alpha = 1$, the forecast for the next observation is the most recent observation (i.e. all weight assigned to the latest observed value and no weight to any of the previous observations).\n",
    "\n",
    "##### 2. Holt's Method\n",
    "\n",
    "- Extension of the SES method to account for trend (but not seasonality) in a time series\n",
    "- We use a trend-smoothing factor, $\\beta$ which takes a value between $0$ and $1$. In `statsmodels`, its called `smoothing_slope`.\n",
    "- We can also allow for a damping of the trend over time using the flag `damped=True`.\n",
    "\n",
    "\n",
    "We implement exponential smoothing and forecasting on the end-of-month `Asian Paints` stock prices using both the methods discussed.\n",
    "\n",
    "We employ the data from 2009 to 2018 as part of our training set and that of 2019 & early 2020 as our test set.\n",
    "\n",
    "> *Asian Paints Limited, together with its subsidiaries, manufactures, sells, and distributes paints and coatings for decorative and industrial use in India and internationally. It operates in the Paints and Home Improvement segments.* - [Source](https://finance.yahoo.com/quote/ASIANPAINT.NS/profile?p=ASIANPAINT.NS).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end7 = datetime.date(2020, 4, 1)\n",
    "start7 = datetime.date(2009, 1, 1)\n",
    "ticker7 = \"ASIANPAINT.NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker7, start=start7, end=end7, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker7} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(f\"{ticker7}.csv\")\n",
    "# df4 = pd.read_csv(\"gold_prices_inr.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling to obtain monthly stock prices with the following rules\n",
    "## 'Open': first opening price of the month\n",
    "## 'High': max price of the month\n",
    "## 'Low': min price of the month\n",
    "## 'Close' and 'Adj Close': last closing price of the month\n",
    "\n",
    "df7 = df7.resample('M').agg({'Open':'first', 'High':'max', 'Low': 'min', \n",
    "                             'Close':'last', 'Adj Close':'last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\"], inplace=True)\n",
    "df7.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df7.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start7_str = start7.strftime(\"%b. %Y\")\n",
    "end7_str = (end7 - pd.Timedelta(\"3 days\")).strftime(\"%b. %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "df7['adj_close'].plot(figsize=(12, 8), title=f\"{ticker7} monthly adjusted close prices ({start7_str} - {end7_str})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the training & test set\n",
    "\n",
    "training_index = df7.index.year < 2019\n",
    "df7_train = df7[training_index]\n",
    "df7_test = df7[~training_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***********************************************************\n",
    "## ***** EXPERIMENTAL : IGNORE THIS CELL *********************\n",
    "## ***********************************************************\n",
    "\n",
    "## Creating the training & test set\n",
    "\n",
    "# train_length = int(np.round(0.9 * df7.shape[0]))\n",
    "# df7_train = df7.iloc[:train_length]\n",
    "# df7_test = df7.iloc[train_length:]\n",
    "# test_length = len(df7) - len(df7_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = len(df7_train)\n",
    "test_length = len(df7_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length, test_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7_train.shape, df7_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***********************************************************\n",
    "## ***** EXPERIMENTAL : IGNORE THIS CELL *********************\n",
    "## ***********************************************************\n",
    "\n",
    "\n",
    "# models__ = [ses_fit_1, ses_fit_2]\n",
    "# forecasts__ = [ses_fit_1_yhat, ses_fit_2_yhat]\n",
    "\n",
    "# for each_model, each_forecast in zip(models__, forecasts__):\n",
    "#     each_forecast = each_model.forecast(test_length)\n",
    "#     each_forecast.index = df7_test.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##########################################################\n",
    "######### Simple Exponential Smoothing ###################\n",
    "##########################################################\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "\n",
    "## Creating the SES model class\n",
    "ses_model = SimpleExpSmoothing(df7_train)\n",
    "\n",
    "## Fitting and forecasting models at different smoothing levels\n",
    "ses_fit1 = ses_model.fit(smoothing_level=0.2, optimized=False)\n",
    "ses_fcast1 = ses_fit1.forecast(test_length).rename(r'$\\alpha=0.2$')\n",
    "\n",
    "ses_fit2 = ses_model.fit(smoothing_level=0.6, optimized=False)\n",
    "ses_fcast2 = ses_fit2.forecast(test_length).rename(r'$\\alpha=0.6$')\n",
    "\n",
    "## The recommended way of model fitting and forecasting\n",
    "ses_fit3 = ses_model.fit()\n",
    "ses_fcast3 = ses_fit3.forecast(test_length).rename(r'$\\alpha=%.3f$' \\\n",
    "                                                      %ses_fit3.model.params['smoothing_level'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(f\"Simple Exponential Smoothing of the {ticker7} stock price\")\n",
    "observed, = plt.plot(df7.adj_close, color='black')\n",
    "\n",
    "plt.plot(ses_fit1.fittedvalues, color='blue')\n",
    "line1, = plt.plot(ses_fcast1, color='blue')\n",
    "\n",
    "plt.plot(ses_fit2.fittedvalues, color='green')\n",
    "line2, = plt.plot(ses_fcast2, color='green')\n",
    "\n",
    "plt.plot(ses_fit3.fittedvalues, color='red')\n",
    "line3, = plt.plot(ses_fcast3, color='red')\n",
    "plt.legend([observed, line1, line2, line3], [\"observed\", ses_fcast1.name, ses_fcast2.name, ses_fcast3.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "\n",
    "- All the forecasts are flat lines.\n",
    "- We expect to see such a plot, since by defintion, SES assumes no trend.\n",
    "- We manually selected the `smoothing_level` for the first two models.\n",
    "- In the third model, we let `statsmodel` automatically find the optimized value of the smoothing factor. Notice that it eventually calculated a value almost equal to 1. That is to say, the best forecast for the price in the next period is the last observed price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***********************************************************\n",
    "## ***** EXPERIMENTAL : IGNORE THIS CELL *********************\n",
    "## ***********************************************************\n",
    "\n",
    "\n",
    "# ## Forecasting and index setting for all cases\n",
    "# models_ = [ses_fit_1, ses_fit_2, ses_fit_3]\n",
    "# forecasts_ = [ses_fit_1_yhat, ses_fit_2_yhat, ses_fit_3_yhat]\n",
    "\n",
    "# for each_model in models_:\n",
    "#     each_forecast = each_model.forecast(test_length)\n",
    "#     #each_forecast.index = df7_test.index\n",
    "\n",
    "# # for each_forecast in forecasts_:\n",
    "# #     each_forecast.index = df7_test.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##########################################################\n",
    "################### Holt's Method ########################\n",
    "##########################################################\n",
    "\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, Holt\n",
    "\n",
    "## Creating, fitting and forecasting 3 different models using Holt's method\n",
    "\n",
    "## creating\n",
    "holt_model1 = Holt(df7_train)\n",
    "holt_model2 = Holt(df7_train, exponential=True)\n",
    "holt_model3 = Holt(df7_train, damped=True)\n",
    "\n",
    "## fitting and forecasting\n",
    "\n",
    "## Version I: Additive trend model, with α = 0.8 and β = 0.2\n",
    "holt_fit1 = holt_model1.fit(smoothing_level=0.8, smoothing_slope=0.2, optimized=False)\n",
    "holt_fcast1 = holt_fit1.forecast(test_length).rename(\"Holt's linear trend\")\n",
    "\n",
    "## Version II: Exponential trend model, with α = 0.8 and β = 0.2\n",
    "holt_fit2 = holt_model2.fit(smoothing_level=0.8, smoothing_slope=0.2, optimized=False)\n",
    "holt_fcast2 = holt_fit2.forecast(test_length).rename(\"Exponential trend\")\n",
    "\n",
    "## Version III: Additive trend model, with α = 0.8 and β = 0.2 and allowing for \n",
    "## optimization of the dampening parameter ϕ\n",
    "holt_fit3 = holt_model3.fit(smoothing_level=0.8, smoothing_slope=0.2)\n",
    "holt_fcast3 = holt_fit3.forecast(test_length).rename(\"Additive damped trend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(f\"Holt's smoothing of the {ticker7} stock price\")\n",
    "observed, = plt.plot(df7.adj_close, color='black')\n",
    "\n",
    "plt.plot(holt_fit1.fittedvalues, color='blue')\n",
    "line1, = plt.plot(holt_fcast1, color='blue')\n",
    "\n",
    "plt.plot(holt_fit2.fittedvalues, color='green')\n",
    "line2, = plt.plot(holt_fcast2, color='green')\n",
    "\n",
    "plt.plot(holt_fit3.fittedvalues, color='red')\n",
    "line3, = plt.plot(holt_fcast3, color='red')\n",
    "plt.legend([observed, line1, line2, line3], [\"observed\", holt_fcast1.name, holt_fcast2.name, holt_fcast3.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "\n",
    "- All the forecasts are improvements over the SES methods.\n",
    "- We manually set the values of $\\alpha$ and $\\beta$ in all three models.\n",
    "- In `Holt's linear trend`, we chose the trend to be additive (i.e. linear) by default.\n",
    "- In `Exponential trend`, we explicitly set the trend to be exponential by setting `exponential=True`.\n",
    "- In `Additive damped trend`, we used a damped version of Holt's linear model by setting `damped=True`. We allowed `statsmodel` to automatically find the optimized value of the dampening factor $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method IV : Modeling time series using ARIMA models\n",
    "\n",
    "The **ARIMA (AutoRegressive Integrated Moving Average)** class of models is a popular statistical technique in time series forecasting. It exploits different standard temporal structures seen in time series processes.\n",
    "\n",
    "> *Exponential smoothing and ARIMA models are the two most widely used approaches to time series forecasting, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.* - [Source](#hyndman)\n",
    "\n",
    "##### ***Crash course on ARIMA***\n",
    "We will now take a brief look at its key features by taking apart the acronym.\n",
    "\n",
    "1. **Auto Regressive (AR)**:\n",
    "\n",
    "- Regression of a time series process onto itself (its past versions)\n",
    "- A time series process is $AR$ if its present value depends on a linear combination of past observations.\n",
    "- In financial time series, an $AR$ model attempts to explain the mean reversion and trending behaviours that we observe in asset prices.\n",
    "\n",
    "2. **Integrated (I)**:\n",
    "\n",
    "For a time series process ${X_t}$ recorded at regular intervals, the difference operation is defined as $$\\nabla X_t = X_t - X_{t-1}$$\n",
    "\n",
    "The difference operator (denoted by $\\nabla$) can be applied repeatedly. For example, \n",
    "$$\\nabla^2 X_t = \\nabla X_t - \\nabla X_{t-1}$$\n",
    "\n",
    "- A time series process is integrated of order $d$ (denoted by $I(d)$), if differencing the observations $d$ times, makes the process stationary.\n",
    "\n",
    "3. **Moving Average (MA)**:\n",
    "\n",
    "-  A time series process is $MA$ if its present value can be written as a linear combination of past error terms.\n",
    "- $MA$ models try to capture the idiosyncratic shocks observed in financial markets. We can think of events like terrorist attacks, earnings surprises, sudden political changes, etc. as the random shocks affecting the asset price movements.\n",
    "\n",
    "When we use the ARIMA class to model a time series process, each of the above components are specified in the model as parameters (with the notations $p$, $d$, and $q$ respectively). \n",
    "\n",
    "That is, the classification $ARIMA(p, d, q)$ process can be thought of as $$AR(p)I(d)MA(q)$$ \n",
    "Here,\n",
    "\n",
    "1. $p$: The number of past observations (we usually call them *lagged terms*) of the process included in the model.\n",
    "2. $d$: The number of times we difference the original process to make it stationary.\n",
    "3. $q$: The number of past error terms (we usually call them *lagged error terms* or *lagged residuals*) of the process included in the model.\n",
    "\n",
    "When we model our time series process with the ARIMA class, we implicitly assume that the underlying data generating process (and by extension the observations we record) is an ARIMA process. \n",
    "\n",
    "We should validate our assumptions (especially the implicit ones which slip under the radar) and recognize the limitations of our models. A well-known deficiency of ARIMA applications on financial time series is its failure to capture the phenomenon of volatility clustering. However, despite their inaccurate point estimates, they give rise to informative confidence intervals.\n",
    "\n",
    "We now fit an ARIMA model to the weekly stock prices (from mid-2010 to mid-2019) of `Netflix` and learn to evaluate it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start8 = datetime.date(2010, 6, 30)\n",
    "end8 = datetime.date(2019, 7, 1)\n",
    "ticker8 = \"NFLX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker8, start=start8, end=end8, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker8} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(f\"{ticker8}.csv\")\n",
    "# df4 = pd.read_csv(\"NFLX.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling to obtain weekly stock prices with the following rules\n",
    "## 'Open': first opening price of the month\n",
    "## 'High': max price of the month\n",
    "## 'Low': min price of the month\n",
    "## 'Close' and 'Adj Close': last closing price of the month\n",
    "\n",
    "df8 = df8.resample('W').agg({'Open':'first', 'High':'max', 'Low': 'min', \n",
    "                             'Close':'last', 'Adj Close':'last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\"], inplace=True)\n",
    "df8.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df8.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for null values\n",
    "\n",
    "df8[df8['adj_close'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start8_str = (start8 + pd.Timedelta(\"5 days\")).strftime(\"%B %Y\")\n",
    "end8_str = (end8 - pd.Timedelta(\"5 days\")).strftime(\"%B %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "df8['adj_close'].plot(figsize=(12, 8), title=f\"{ticker8} weekly adjusted close prices ({start8_str} - {end8_str})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "check_stationarity(df8['adj_close'], wl1=4, wl2=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The p-value is nearly 1 (and equivalently the test statistic is greater than the critical values at all 3 significance levels). So the ADF test result is that the price series is non-stationary.\n",
    "- The rolling means and volatility plots are time-varying. So we arrive at the same conclusion by examining the plots.\n",
    "- From the ACF, there are significant autocorrelations above the 95% confidence interval at all lags. From the PACF, we have spikes at lags 1, 8, 9, 13, 18, 23 and 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8['log_returns'] = np.log(df8['adj_close'] / df8['adj_close'].shift(1))\n",
    "df8.dropna(axis='rows', how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "check_stationarity(df8['log_returns'], wl1=4, wl2=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- As per the ADF test results, the `Netflix` returns are stationary since the p-value is almost 0 and the test statistic is less than all the critical values.\n",
    "- The returns and rolling means of the returns are all centred around 0. As the time scale increases, the means become more and more constant. At shorter time scales, the noise tends to obscure the signal.\n",
    "- The volatily is time-varying at both the faster and slower rolling levels.\n",
    "- We can see bristles near or beyond the blue shadow at lags 17 and 26 in the ACF plot and lags 12, 16, 17, 18 and 26 in the PACF plot.\n",
    "- **Returns are log price differences. So we can also infer from the above two checks, that the price series is integrated with order $1.$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##########################################################\n",
    "################ ARIMA model fitting #####################\n",
    "##########################################################\n",
    "import scipy.stats as scs\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "## Creating the ARIMA model class\n",
    "## We select the order arbitrarily (p and q)\n",
    "## We inferred d from the results of `check_stationarity`\n",
    "\n",
    "## Defining the model by providing the training set and providing the parameters p, d, q\n",
    "arima_model = ARIMA(df8['adj_close'], order=(3, 1, 2))\n",
    "\n",
    "## Fitting the model, disp=0 is to switch off verbose display\n",
    "arima_fit1 = arima_model.fit(disp=0)\n",
    "\n",
    "## Printing a summary of the model\n",
    "arima_fit1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- We chose an $ARIMA(3, 1, 2)$ model to fit the price series of `Netflix`. Equivalently, we could have fit an $ARIMA(3, 0, 2)$ to the returns instead. \n",
    "- The `summary()` method provides the results of the model fitting exercise on the **in-sample data set** (a.k.a. the training data).\n",
    "- The most important part is the table at the centre which has the coefficient values, their 95% confidence intervals and their corresponding p-values.\n",
    "- However, we also need to run model diagnostics by examining the residual errors closely. This will tell us if our model was a good fit to the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_diagnostics(resids_, figsize=(15, 9), n_lags=40):\n",
    "    '''\n",
    "    Diagnoses the fit of an ARIMA model by examining its residuals.\n",
    "    Returns a chart with with multiple plots\n",
    "    '''\n",
    "    # Creating placeholder subplots\n",
    "    M = 2\n",
    "    N = 2\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(M, N, figsize=figsize)\n",
    "\n",
    "    r = resids_\n",
    "    resids_ = (r - np.nanmean(r)) / np.nanstd(r)\n",
    "    resids_nonmissing = resids_[~(np.isnan(resids_))]\n",
    "    \n",
    "    # Plotting residuals over time\n",
    "    sns.lineplot(x=np.arange(len(resids_)), \n",
    "                 y=resids_, ax=ax1)\n",
    "    ax1.set_title('Standardized residuals')\n",
    "\n",
    "    # Plotting the distribution of residuals\n",
    "    x_lim = (-1.96 * 2, 1.96 * 2)\n",
    "    r_range = np.linspace(x_lim[0], x_lim[1])\n",
    "    norm_pdf = scs.norm.pdf(r_range)\n",
    "    \n",
    "    sns.distplot(resids_nonmissing, hist=True, kde=True, \n",
    "                 norm_hist=True, ax=ax2)\n",
    "    ax2.plot(r_range, norm_pdf, color='green', linewidth=2, label='N(0,1)')\n",
    "    ax2.set_title('Distribution of standardized residuals')\n",
    "    ax2.set_xlim(x_lim)\n",
    "    ax2.legend()\n",
    "        \n",
    "    # Q-Q plot\n",
    "    qq = sm.qqplot(resids_nonmissing, line='s', ax=ax3)\n",
    "    ## 's' is for standardized line to compare the plot with a normal distribution\n",
    "    ax3.set_title('Q-Q plot')\n",
    "\n",
    "    # ACF plot\n",
    "    sm.graphics.tsa.plot_acf(resids_, lags=n_lags, ax=ax4, alpha=0.05)\n",
    "    ax4.set_title('ACF plot')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(font_scale=1.2)\n",
    "arima_diagnostics(arima_fit1.resid)\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "    \n",
    "- `Standardized residuals`: The mean of the residuals is approximately zero. However, it's variance is much higher in the second half of the series.\n",
    "- `Distribution of standardized residuals` and `Q-Q plot`: Both plots indicate fatter tails compared to a normal distribution.\n",
    "- `ACF plot`: There seems to be serial correlations at lags 8, 13, 14, 22 and a few more. \n",
    "- **If the fit is good, we should see residuals similar to Gaussian white noise. It's not so here.**\n",
    "- So we can infer that the model is not a very good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical tests we additionally run\n",
    "\n",
    "1. *To check for autocorrelations in residuals: [`The Ljung-Box test`](https://en.wikipedia.org/wiki/Ljung%E2%80%93Box_test)*\n",
    "\n",
    "The null hypothesis is that the serial correlations of the time series are zero. We use it in addition to visual interpretation of ACF/PACF plots.\n",
    "\n",
    "2. *To check for normality in residuals: [`The Jarque-Bera test`](https://en.wikipedia.org/wiki/Jarque-Bera_test)*\n",
    "\n",
    "The null hypothesis is that the time series is normally distributed. We use it in addition to visual interpretation of plots like the residual distribution and the Q-Q plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the Ljung-Box test and plotting the results\n",
    "\n",
    "ljung_box_results = sm.stats.acorr_ljungbox(arima_fit1.resid)\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "sns.scatterplot(x=range(len(ljung_box_results[1])), y=ljung_box_results[1], ax=ax)\n",
    "ax.axhline(0.05, ls='--', color='red')\n",
    "ax.set(title=f\"Ljung-Box test results (after modeling {ticker8} stock prices)\", xlabel='Lags', ylabel='p-value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "    \n",
    "- There are no significant serial correlations until lag 12.\n",
    "- However, many of the correlations from lag 13 are below the red line.\n",
    "- So our model is not a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the Jarque-Bera test and interpreting its results\n",
    "\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "jb_test_stat, pvalue, _, _ = jarque_bera(arima_fit1.resid)\n",
    "print(f\"Jarque-Bera statistic: {jb_test_stat:.2f} with p-value: {pvalue:.2f}\")\n",
    "\n",
    "if pvalue < 0.05:\n",
    "    print(\"Our residuals are likely not normally distributed.\")\n",
    "else:\n",
    "    print(\"Our residuals are likely normally distributed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***********************************************************\n",
    "## ******* Manually checking for best ARIMA ******************\n",
    "## ***********************************************************\n",
    "\n",
    "############## Please try on your own time ##################\n",
    "## Make necessary modifications (if needed) to the below code\n",
    "\n",
    "\n",
    "# %%time\n",
    "\n",
    "# best_aic = np.inf\n",
    "# best_order = None\n",
    "# best_mdl = None\n",
    "\n",
    "# pq_rng = range(5)\n",
    "# d_rng = range(2)\n",
    "# for i in pq_rng:\n",
    "#     for d in d_rng:\n",
    "#         for j in pq_rng:\n",
    "#             try:\n",
    "#                 tmp_mdl = ARIMA(df8['adj_close'], order=(i, d, j)).fit(method='mle', trend='nc')\n",
    "#                 tmp_aic = tmp_mdl.aic\n",
    "#                 if tmp_aic < best_aic:\n",
    "#                     best_aic = tmp_aic\n",
    "#                     best_order = (i, d, j)\n",
    "#                     best_mdl = tmp_mdl\n",
    "#             except:\n",
    "#                 continue\n",
    "            \n",
    "# print(\"aic : \",best_aic, \"| order : \",best_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automatically finding the best ARIMA fit (using the [`pmdarima`](https://alkaline-ml.com/pmdarima/develop/about.html) library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pmdarima as pm\n",
    "\n",
    "## Fitting the model (This is the default setting)\n",
    "arima_fit2 = pm.auto_arima(df8['adj_close'], error_action='ignore', \n",
    "                           suppress_warnings=True, seasonal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing a summary of the model\n",
    "arima_fit2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Fitting the model(With more tuning of the parameters)\n",
    "arima_fit3 = pm.auto_arima(df8['adj_close'], error_action='ignore', \n",
    "                           suppress_warnings=True, stepwise=False, \n",
    "                           approximation=False, seasonal=False)\n",
    "\n",
    "arima_fit3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "\n",
    "- The most suitable model is $ARIMA(2, 1, 2)$\n",
    "- Our guiding principle when we build models is [Occam's Razor](http://pespmc1.vub.ac.be/OCCAMRAZ.html) i.e. we want a model with the fewest parameters that can explain our time series process. \n",
    "- We therefore use information criterion ([Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion)).\n",
    "- When choosing from multiple competing models, we choose the one which has the smallest AIC.\n",
    "- The idea is to find the right balance between underfitting and overfitting. AIC helps us find that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting using the ARIMA class\n",
    "\n",
    "We will forecast using both, $ARIMA(3, 1, 2)$ and $ARIMA(2, 1, 2)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start9 = datetime.date(2019, 6, 30)\n",
    "end9 = datetime.date(2020, 7, 7)\n",
    "ticker9 = ticker8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker9, start=start9, end=end9, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker9} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(f\"{ticker8}2.csv\")\n",
    "# df4 = pd.read_csv(\"NFLX.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling to obtain weekly stock prices with the following rules\n",
    "## 'Open': first opening price of the month\n",
    "## 'High': max price of the month\n",
    "## 'Low': min price of the month\n",
    "## 'Close' and 'Adj Close': last closing price of the month\n",
    "\n",
    "df9 = df9.resample('W').agg({'Open':'first', 'High':'max', 'Low': 'min', \n",
    "                             'Close':'last', 'Adj Close':'last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\"], inplace=True)\n",
    "df9.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df9.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for null values\n",
    "\n",
    "df9[df9['adj_close'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Forecasting using the first model, ARIMA(3, 1, 2)\n",
    "\n",
    "n_fcast1 = len(df9)\n",
    "arima_fcast1 = arima_fit1.forecast(n_fcast1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arima_fcast1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fcast1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "arima_fcast1 = [pd.DataFrame(arima_fcast1[0], columns=['forecast']), \n",
    "                pd.DataFrame(arima_fcast1[2], columns=['lower_95', \n",
    "                                                       'upper_95'])]\n",
    "\n",
    "arima_fcast1 = pd.concat(arima_fcast1, axis=1).set_index(df9.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Forecasting using the second model, ARIMA(2, 1, 2)\n",
    "\n",
    "arima_fcast3 = arima_fit3.predict(n_periods=n_fcast1, \n",
    "                                  return_conf_int=True, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "arima_fcast3 = [pd.DataFrame(arima_fcast3[0], columns=['prediction']), \n",
    "                pd.DataFrame(arima_fcast3[1], columns=['lower_95', \n",
    "                                                       'upper_95'])]\n",
    "\n",
    "arima_fcast3 = pd.concat(arima_fcast3, axis=1).set_index(df9.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Plotting the results for both models\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "\n",
    "ax = sns.lineplot(data=df9['adj_close'], color='black', label='Actual')\n",
    "\n",
    "ax.plot(arima_fcast1.forecast, color='green', label='ARIMA(3, 1, 2)')\n",
    "\n",
    "ax.fill_between(arima_fcast1.index, arima_fcast1.lower_95, \n",
    "                arima_fcast1.upper_95, alpha=0.3, \n",
    "                facecolor='green')\n",
    "\n",
    "ax.plot(arima_fcast3.prediction, color='red', label='ARIMA(2, 1, 2)')\n",
    "\n",
    "ax.fill_between(arima_fcast3.index, arima_fcast3.lower_95, \n",
    "                arima_fcast3.upper_95, alpha=0.2, \n",
    "                facecolor='red')\n",
    "\n",
    "ax.set(title=f\"{ticker8} stock price - actual vs. predicted\", xlabel='Date',\n",
    "       ylabel='Adjusted close price (US$)')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/ch3_im25.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief look at modeling volatility using the ARCH/GARCH family of models\n",
    "\n",
    "The ARIMA class of models is widely used in asset price forecasting. However, as we have seen, time-varying volatility and volatility clustering (heteroskedasticity) are recurrent themes in finance. The ARMA/ARIMA models do not account for it. Fortunately, we have the ARCH/GARCH method which allows us to model for the time-dependent change in the volatility of a time-series.\n",
    "\n",
    "The $ARIMA + GARCH$ combination is used to improve forecasts. In practice, we jointly estimate the mean returns and the volatility associated with the returns.\n",
    "\n",
    "We model conditional variance in Python with the `arch` library.\n",
    "\n",
    "## Autoregressive Conditionally Heteroskedastic Models (ARCH)\n",
    "\n",
    "If we observe the name of the model, we can make a pretty good guess what it does. It estimates the conditional variance($\\sigma_t$ in the below setup) over time based on the past values of the variance (hence the name autoregressive). We now consider the $ARCH(1)$ model below which can be easily generalized to $ARCH(q)$.\n",
    "\n",
    "$$r_t = \\mu + \\epsilon_t$$ Above, we express returns as a (multiplicative) combination of deterministic and stochastic components.\n",
    "$$\\epsilon_t = \\sigma_t w_t$$ Carrying on from the previous step, we express the stochastic component as a combination of Gaussian white noise $w_t$ and the conditional standard deviation $\\sigma_t$.\n",
    "<div class=\"alert alert-success\">$$\\sigma_{t}^2 = \\alpha_0 + \\alpha_1 \\epsilon_{t-1}^2$$</div> \n",
    "\n",
    "***This is the ARCH model where $\\alpha_0$ and $\\alpha_1$ are its parameters.***\n",
    "\n",
    "It is useful to think of $ARCH(q)$ as the application of $AR(p)$ to the variance of a time series process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start10 = datetime.date(2015, 1, 1)\n",
    "end10 = datetime.date(2020, 10, 10)\n",
    "ticker10 = \"HINDUNILVR.NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker10, start=start10, end=end10, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker10} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], inplace=True)\n",
    "df10.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df10.tail())\n",
    "\n",
    "simple_perc_returns = 100 * df10['adj_close'].pct_change().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_perc_returns.plot(title=f'Daily percentage returns of {ticker10}: {start10} to {end10}', figsize=(12, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arch\n",
    "\n",
    "# Specifying an ARCH(1) model \n",
    "arch_model1 = arch.arch_model(simple_perc_returns, vol='ARCH', mean=\"Zero\", p=1, o=0, q=0)\n",
    "\n",
    "# Estimating the model\n",
    "arch_model1_fitted = arch_model1.fit()\n",
    "\n",
    "# Printing the summary\n",
    "print(arch_model1_fitted.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-period out-of-sample forecast\n",
    "arch_model1_forecast = arch_model1_fitted.forecast(horizon=1)\n",
    "print(arch_model1_forecast.mean['h.1'].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_model1_fitted.plot(annualize='D');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying a GARCH(1, 1) model \n",
    "garch_model1 = arch.arch_model(simple_perc_returns, vol='GARCH', mean=\"Zero\", p=1, o=0, q=1)\n",
    "\n",
    "# Estimating the model\n",
    "garch_model1_fitted = garch_model1.fit()\n",
    "\n",
    "# Printing the summary\n",
    "print(garch_model1_fitted.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_model1_fitted.plot(annualize='D');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pmdarima as pm\n",
    "\n",
    "## fitting ARIMA on adjusted close prices\n",
    "arima_fit_10 = pm.auto_arima(df10['adj_close'], error_action='ignore', \n",
    "                           suppress_warnings=True, seasonal=False)\n",
    "\n",
    "p, d, q = arima_fit_10.order\n",
    "print(p, d, q)\n",
    "arima_fit_residuals = arima_fit_10.arima_res_.resid\n",
    "\n",
    "# fitting a GARCH(1,1) model after fitting ARIMA on the prices\n",
    "garch_model_2 = arch.arch_model(arima_fit_residuals, p=1, o=0, q=1)\n",
    "garch_model2_fitted = garch_model_2.fit()\n",
    "\n",
    "# Using ARIMA to predict prices\n",
    "predicted_prices = arima_fit_10.predict(n_periods=1)[0]\n",
    "\n",
    "# Using GARCH to predict the residuals\n",
    "garch_model_2_forecast = garch_model2_fitted.forecast(horizon=1)\n",
    "predicted_residual = garch_model_2_forecast.mean['h.1'].iloc[-1]\n",
    "# Combining both models\n",
    "predicted_price_range = (predicted_prices-predicted_residual, predicted_prices+predicted_residual)\n",
    "\n",
    "print(predicted_price_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'references'></a>\n",
    "#### References\n",
    "<a id = 'bnshephard'></a>\n",
    "<a id = 'arch'></a>\n",
    "<a id = 'others'></a>\n",
    "<a id = 'eryk'></a>\n",
    "<a id = 'cont2001'></a>\n",
    "<a id = 'hyndman'></a>\n",
    "\n",
    "1. Brownlee, Jason. Introduction to time series forecasting with python: how to prepare data and develop models to predict the future. Machine Learning Mastery (2018).\n",
    "2. Cochrane, John H. \"Time series for macroeconomics and finance.\" Manuscript, University of Chicago (2005).\n",
    "3. Cont, R. Empirical properties of asset returns: stylized facts and statistical issues (2001).\n",
    "4. https://towardsdatascience.com/@eryk.lewinson\n",
    "5. https://pyflux.readthedocs.io/en/latest/getting_started.html\n",
    "6. https://tomaugspurger.github.io/modern-7-timeseries\n",
    "7. https://arch.readthedocs.io/en/latest/univariate/univariate_volatility_modeling.html\n",
    "8. https://www.statsmodels.org/devel/examples/notebooks/generated/exponential_smoothing.html\n",
    "9. Hyndman, R.J., & Athanasopoulos, G. Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on 11 July 2020.\n",
    "6. Tsay, Ruey S. Analysis of financial time series. Vol. 543. John Wiley & Sons (2005).\n",
    "7. Campbell, John Y., Andrew Wen-Chuan Lo, and Craig MacKinlay. The Econometrics of Financial Markets. Vol. 2. Princeton, NJ: princeton University press (1997).\n",
    "10. http://www.blackarbs.com/blog/time-series-analysis-in-python-linear-models-to-garch/11/1/2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
