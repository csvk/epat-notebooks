{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:50:52.229158Z",
     "start_time": "2021-03-29T23:50:46.298110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-29T20:50:46-03:00\n",
      "\n",
      "CPython 3.8.6\n",
      "IPython 7.19.0\n",
      "\n",
      "compiler   : MSC v.1916 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n",
      "\n",
      " blackarbs_algo_strategy_dev\n",
      "\n",
      "CPython 3.8.6\n",
      "IPython 7.19.0\n",
      "\n",
      "numpy 1.19.4\n",
      "pandas 1.2.1\n",
      "scipy 1.5.3\n",
      "sklearn 0.23.2\n",
      "mlfinlab not installed\n",
      "seaborn 0.11.0\n",
      "matplotlib 3.3.3\n",
      "\n",
      "compiler   : MSC v.1916 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n",
      "Git hash   : 8bad65bc7a7f597f230fc8c0154640db2a5a0ecd\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import standard libs\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace as debug\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "\n",
    "# import python scientific stack\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import numba as nb\n",
    "\n",
    "# import ffn\n",
    "import yfinance as yf\n",
    "import bottleneck as bk\n",
    "import mlxtend as mlx\n",
    "\n",
    "# import mlfinlab as ml\n",
    "from boruta import BorutaPy as bp\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    "    # StackingClassifier,\n",
    ")\n",
    "from mlxtend.classifier import StackingClassifier  # <-- works w/o errors\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    make_scorer,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "import numpy_ext as npx\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\"\"\"from mlfinlab.feature_importance import (\n",
    "    mean_decrease_impurity,\n",
    "    mean_decrease_accuracy,\n",
    "    single_feature_importance,\n",
    "    plot_feature_importance,\n",
    ")\n",
    "from mlfinlab.feature_importance import ClassificationModelFingerprint\n",
    "from mlfinlab.ensemble import SequentiallyBootstrappedBaggingClassifier\"\"\"\n",
    "import shap\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# import util libs\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import missingno as msno\n",
    "\n",
    "from algo_dev.CONSTANTS import REPO_NAME\n",
    "from algo_dev.tools.plots import *\n",
    "from algo_dev.tools.utils import *\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# THESE ARE VARIABLES FOR EASILY ACCESSING DIFFERENT\n",
    "# DIRECTORIES FOR ACCESSING AND SAVING DATA AND IMAGES\n",
    "# IF NECESSARY. CHANGE THEM TO MATCH YOUR DIRECTORY\n",
    "# STRUCTURE.\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# REPO_NAME = \"blackarbs_algo_strategy_dev\"\n",
    "print(\"\\n\", REPO_NAME)\n",
    "project_dir = get_relative_project_dir(REPO_NAME)\n",
    "data_dir = project_dir / \"data\"\n",
    "external = data_dir / \"external\"\n",
    "processed = data_dir / \"processed\"\n",
    "viz = project_dir / \"viz\"\n",
    "\n",
    "print()\n",
    "%watermark -v -m -p numpy,pandas,scipy,sklearn,mlfinlab,seaborn,matplotlib -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:50:52.571347Z",
     "start_time": "2021-03-29T23:50:52.231150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nsns_params = {\\n    \\\"xtick.major.size\\\": 2,\\n    \\\"ytick.major.size\\\": 2,\\n    \\\"font.size\\\": 12,\\n    \\\"font.weight\\\": \\\"medium\\\",\\n    \\\"figure.figsize\\\": (10, 7),\\n    \\\"font.family\\\": \\\"Ubuntu Mono\\\",\\n}\\n\\nsns.set_style(\\\"white\\\", sns_params)\\nsns.set_context(sns_params)\\nsavefig_kwds = dict(dpi=90, bbox_inches=\\\"tight\\\", frameon=True, format=\\\"png\\\")\\n\\n\\nfrom jupyterthemes import jtplot\\n\\njtplot.style(\\n    theme=\\\"grade3\\\",  # \\\"oceans16\\\",  # \\\"monokai\\\",\\n    context=\\\"talk\\\",\\n    fscale=1.4,\\n    ticks=True,\\n    spines=False,\\n    grid=True,\\n    gridlines=\\\"--\\\",\\n)\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nsns_params = {\\n    \\\"xtick.major.size\\\": 2,\\n    \\\"ytick.major.size\\\": 2,\\n    \\\"font.size\\\": 12,\\n    \\\"font.weight\\\": \\\"medium\\\",\\n    \\\"figure.figsize\\\": (10, 7),\\n    \\\"font.family\\\": \\\"Ubuntu Mono\\\",\\n}\\n\\nsns.set_style(\\\"white\\\", sns_params)\\nsns.set_context(sns_params)\\nsavefig_kwds = dict(dpi=90, bbox_inches=\\\"tight\\\", frameon=True, format=\\\"png\\\")\\n\\n\\nfrom jupyterthemes import jtplot\\n\\njtplot.style(\\n    theme=\\\"grade3\\\",  # \\\"oceans16\\\",  # \\\"monokai\\\",\\n    context=\\\"talk\\\",\\n    fscale=1.4,\\n    ticks=True,\\n    spines=False,\\n    grid=True,\\n    gridlines=\\\"--\\\",\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "sns_params = {\n",
    "    \"xtick.major.size\": 2,\n",
    "    \"ytick.major.size\": 2,\n",
    "    \"font.size\": 12,\n",
    "    \"font.weight\": \"medium\",\n",
    "    \"figure.figsize\": (10, 7),\n",
    "    \"font.family\": \"Ubuntu Mono\",\n",
    "}\n",
    "\n",
    "sns.set_style(\"white\", sns_params)\n",
    "sns.set_context(sns_params)\n",
    "savefig_kwds = dict(dpi=90, bbox_inches=\"tight\", frameon=True, format=\"png\")\n",
    "\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(\n",
    "    theme=\"grade3\",  # \"oceans16\",  # \"monokai\",\n",
    "    context=\"talk\",\n",
    "    fscale=1.4,\n",
    "    ticks=True,\n",
    "    spines=False,\n",
    "    grid=True,\n",
    "    gridlines=\"--\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:00.624933Z",
     "start_time": "2021-03-29T23:50:52.574296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "dataframe information\n",
      "-------------------------------------------------------------------------------\n",
      "                       open    high     low   close      up     down   volume\n",
      "datetime                                                                     \n",
      "2019-05-14 12:56:00  283.77  283.86  283.69  283.74  313916   322548   636464\n",
      "2019-05-14 12:57:00  283.74  283.74  283.42  283.44  154415   244134   398549\n",
      "2019-05-14 12:58:00  283.43  283.60  283.40  283.57  240404   316421   556825\n",
      "2019-05-14 12:59:00  283.58  283.59  283.42  283.42  308214   493246   801460\n",
      "2019-05-14 13:00:00  283.42  283.49  283.17  283.32  933738  1283851  2217589\n",
      "--------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1468155 entries, 2004-05-14 06:31:00 to 2019-05-14 13:00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   open    1468155 non-null  float64\n",
      " 1   high    1468155 non-null  float64\n",
      " 2   low     1468155 non-null  float64\n",
      " 3   close   1468155 non-null  float64\n",
      " 4   up      1468155 non-null  int64  \n",
      " 5   down    1468155 non-null  int64  \n",
      " 6   volume  1468155 non-null  int64  \n",
      "dtypes: float64(4), int64(3)\n",
      "memory usage: 89.6 MB\n",
      "None\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"def read_tradestation_futures_data(fn: Path) -> pd.DataFrame:\\n    df = pd.read_csv(fn).rename(str.lower, axis=\\\"columns\\\")\\n    df[\\\"datetime\\\"] = pd.to_datetime(\\n        df[\\\"date\\\"] + \\\" \\\" + df[\\\"time\\\"], infer_datetime_format=True\\n    )\\n    df[\\\"volume\\\"] = df[\\\"up\\\"] + df[\\\"down\\\"]\\n    df.drop([\\\"date\\\", \\\"time\\\"], axis=1, inplace=True)\\n    df.set_index(\\\"datetime\\\", inplace=True)\\n    return df\\n\\n\\nspy = read_tradestation_futures_data(external / \\\"tradestation\\\" / \\\"SPY.txt\\\")\\ncprint(spy)\";\n",
       "                var nbb_formatted_code = \"def read_tradestation_futures_data(fn: Path) -> pd.DataFrame:\\n    df = pd.read_csv(fn).rename(str.lower, axis=\\\"columns\\\")\\n    df[\\\"datetime\\\"] = pd.to_datetime(\\n        df[\\\"date\\\"] + \\\" \\\" + df[\\\"time\\\"], infer_datetime_format=True\\n    )\\n    df[\\\"volume\\\"] = df[\\\"up\\\"] + df[\\\"down\\\"]\\n    df.drop([\\\"date\\\", \\\"time\\\"], axis=1, inplace=True)\\n    df.set_index(\\\"datetime\\\", inplace=True)\\n    return df\\n\\n\\nspy = read_tradestation_futures_data(external / \\\"tradestation\\\" / \\\"SPY.txt\\\")\\ncprint(spy)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_tradestation_futures_data(fn: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fn).rename(str.lower, axis=\"columns\")\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        df[\"date\"] + \" \" + df[\"time\"], infer_datetime_format=True\n",
    "    )\n",
    "    df[\"volume\"] = df[\"up\"] + df[\"down\"]\n",
    "    df.drop([\"date\", \"time\"], axis=1, inplace=True)\n",
    "    df.set_index(\"datetime\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "spy = read_tradestation_futures_data(external / \"tradestation\" / \"SPY.txt\")\n",
    "cprint(spy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:02.909188Z",
     "start_time": "2021-03-29T23:51:00.629921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"def time_consolidator(df, period):\\n    aggregate = {\\n        \\\"open\\\": \\\"first\\\",\\n        \\\"high\\\": \\\"max\\\",\\n        \\\"low\\\": \\\"min\\\",\\n        \\\"close\\\": \\\"last\\\",\\n        \\\"up\\\": \\\"sum\\\",\\n        \\\"down\\\": \\\"sum\\\",\\n        \\\"volume\\\": \\\"sum\\\",\\n    }\\n    return df.resample(f\\\"{period}Min\\\").agg(aggregate).dropna()\\n\\n\\ndef add_basic_features(df):\\n    bars = df.copy()\\n    # bars[\\\"date\\\"] = df.index.strftime(\\\"%Y-%m-%d\\\")\\n    bars[\\\"bar_return\\\"] = bars.close / bars.open - 1\\n    return bars\\n\\n\\ndata = dict()\\ndf = spy.copy()\\nbars_5m = time_consolidator(df, 5)\\nbars_30m = time_consolidator(df, 30)\\nbars_1H = time_consolidator(df, 60)\\nbars_1D = time_consolidator(df, 60 * 24)\\nbars_1W = time_consolidator(df, 60 * 24 * 7)\\n\\n\\\"\\\"\\\"bars_5m = add_basic_features(bars_5m)\\nbars_1H = add_basic_features(bars_1H)\\nbars_1D = add_basic_features(bars_1D)\\nbars_1W = add_basic_features(bars_1W)\\\"\\\"\\\"\\n\\ndata = {\\n    \\\"5m\\\": bars_5m,\\n    \\\"30m\\\": bars_30m,\\n    \\\"1H\\\": bars_1H,\\n    \\\"1D\\\": bars_1D,\\n    \\\"1W\\\": bars_1W,\\n}\";\n",
       "                var nbb_formatted_code = \"def time_consolidator(df, period):\\n    aggregate = {\\n        \\\"open\\\": \\\"first\\\",\\n        \\\"high\\\": \\\"max\\\",\\n        \\\"low\\\": \\\"min\\\",\\n        \\\"close\\\": \\\"last\\\",\\n        \\\"up\\\": \\\"sum\\\",\\n        \\\"down\\\": \\\"sum\\\",\\n        \\\"volume\\\": \\\"sum\\\",\\n    }\\n    return df.resample(f\\\"{period}Min\\\").agg(aggregate).dropna()\\n\\n\\ndef add_basic_features(df):\\n    bars = df.copy()\\n    # bars[\\\"date\\\"] = df.index.strftime(\\\"%Y-%m-%d\\\")\\n    bars[\\\"bar_return\\\"] = bars.close / bars.open - 1\\n    return bars\\n\\n\\ndata = dict()\\ndf = spy.copy()\\nbars_5m = time_consolidator(df, 5)\\nbars_30m = time_consolidator(df, 30)\\nbars_1H = time_consolidator(df, 60)\\nbars_1D = time_consolidator(df, 60 * 24)\\nbars_1W = time_consolidator(df, 60 * 24 * 7)\\n\\n\\\"\\\"\\\"bars_5m = add_basic_features(bars_5m)\\nbars_1H = add_basic_features(bars_1H)\\nbars_1D = add_basic_features(bars_1D)\\nbars_1W = add_basic_features(bars_1W)\\\"\\\"\\\"\\n\\ndata = {\\n    \\\"5m\\\": bars_5m,\\n    \\\"30m\\\": bars_30m,\\n    \\\"1H\\\": bars_1H,\\n    \\\"1D\\\": bars_1D,\\n    \\\"1W\\\": bars_1W,\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def time_consolidator(df, period):\n",
    "    aggregate = {\n",
    "        \"open\": \"first\",\n",
    "        \"high\": \"max\",\n",
    "        \"low\": \"min\",\n",
    "        \"close\": \"last\",\n",
    "        \"up\": \"sum\",\n",
    "        \"down\": \"sum\",\n",
    "        \"volume\": \"sum\",\n",
    "    }\n",
    "    return df.resample(f\"{period}Min\").agg(aggregate).dropna()\n",
    "\n",
    "\n",
    "def add_basic_features(df):\n",
    "    bars = df.copy()\n",
    "    # bars[\"date\"] = df.index.strftime(\"%Y-%m-%d\")\n",
    "    bars[\"bar_return\"] = bars.close / bars.open - 1\n",
    "    return bars\n",
    "\n",
    "\n",
    "data = dict()\n",
    "df = spy.copy()\n",
    "bars_5m = time_consolidator(df, 5)\n",
    "bars_30m = time_consolidator(df, 30)\n",
    "bars_1H = time_consolidator(df, 60)\n",
    "bars_1D = time_consolidator(df, 60 * 24)\n",
    "bars_1W = time_consolidator(df, 60 * 24 * 7)\n",
    "\n",
    "\"\"\"bars_5m = add_basic_features(bars_5m)\n",
    "bars_1H = add_basic_features(bars_1H)\n",
    "bars_1D = add_basic_features(bars_1D)\n",
    "bars_1W = add_basic_features(bars_1W)\"\"\"\n",
    "\n",
    "data = {\n",
    "    \"5m\": bars_5m,\n",
    "    \"30m\": bars_30m,\n",
    "    \"1H\": bars_1H,\n",
    "    \"1D\": bars_1D,\n",
    "    \"1W\": bars_1W,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Functions and Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:03.745614Z",
     "start_time": "2021-03-29T23:51:02.913178Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"#################\\n\\n\\ndef add_log_returns(df: pd.DataFrame, column: str) -> pd.DataFrame:\\n    df[f\\\"log_{column}_return\\\"] = np.log(df[column]).diff()\\n    return df\\n\\n\\n#################\\ndef internal_bar_strength(df: pd.DataFrame) -> float:\\n    return (df.close - df.low) / (df.high - df.low)\\n\\n\\ndef add_internal_bar_strength(df: pd.DataFrame) -> pd.DataFrame:\\n    df[\\\"ibs\\\"] = internal_bar_strength(df)\\n    return df\\n\\n\\n#################\\n# @nb.jit\\ndef aqr_momentum(array: np.ndarray) -> float:\\n    returns = np.diff(np.log(array))  # .diff()\\n    x = np.arange(len(returns))\\n    slope, _, rvalue, _, _ = stats.linregress(x, returns)\\n    return ((1 + slope) ** 252) * (rvalue ** 2)  # annualize slope and multiply by R^2\\n\\n\\n@nb.njit\\ndef aqr_momo_numba(array: np.ndarray) -> float:\\n    y = np.diff(np.log(array))\\n    x = np.arange(y.shape[0])\\n    A = np.column_stack((x, np.ones(x.shape[0])))\\n    model, resid = np.linalg.lstsq(A, y)[:2]\\n    r2 = 1 - resid / (y.size * y.var())\\n    return (((1 + model[0]) ** 252) * r2)[0]\\n\\n\\ndef add_aqr_momentum(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"aqr_momo_{column}_{window}\\\"] = npx.rolling_apply(\\n        aqr_momentum, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\ndef add_aqr_momentum_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"aqr_momo_{column}_{window}\\\"] = npx.rolling_apply(\\n        aqr_momo_numba, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\n#################\\n\\n\\ndef get_slope(array: np.ndarray) -> float:\\n    returns = np.diff(np.log(array))\\n    x = np.arange(len(returns))\\n    slope, _, rvalue, _, _ = stats.linregress(x, returns)\\n    return slope\\n\\n\\n@nb.njit\\ndef get_slope_numba(array: np.ndarray) -> float:\\n    y = np.diff(np.log(array))\\n    # y = y[~np.isnan(y)]\\n    x = np.arange(y.shape[0])\\n    A = np.column_stack((x, np.ones(x.shape[0])))\\n    model, resid = np.linalg.lstsq(A, y)[:2]\\n    return model[0]\\n\\n\\ndef add_slope_column(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"slope_{column}_{window}\\\"] = npx.rolling_apply(\\n        get_slope, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\ndef add_slope_column_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"slope_{column}_{window}\\\"] = npx.rolling_apply(\\n        get_slope_numba, window, df[column].values, n_jobs=1\\n    )\\n    return df\\n\\n\\n#################\\ndef add_average_price(df: pd.DataFrame) -> pd.DataFrame:\\n    df[\\\"average_price\\\"] = (df.high + df.low + df.close + df.open) / 4\\n    return df\\n\\n\\n#################\\ndef add_rolling_min(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    array = npx.rolling_apply(np.min, window, df[column].values, n_jobs=10)\\n    df[f\\\"rmin_{column}_{window}\\\"] = array\\n    return df\\n\\n\\ndef add_rolling_max(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    array = npx.rolling_apply(np.max, window, df[column].values, n_jobs=10)\\n    df[f\\\"rmax_{column}_{window}\\\"] = array\\n    return df\\n\\n\\n#################\\n\\n# for some reason njit is generating zerodivision errors whereas numpy is not\\n@nb.njit\\ndef numba_vwap(\\n    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\\n) -> np.ndarray:\\n    n = np.shape(np.arange(len_df - window))[0]\\n    A = np.empty((n, 2))\\n    for i in np.arange(len_df - window):\\n        tmp_avg = avg[i : i + window]\\n        tmp_v = v[i : i + window]\\n        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\\n        jj = idx[i + window]\\n        A[i, 0] = jj\\n        A[i, 1] = aa\\n    return A\\n\\n\\ndef numpy_vwap(\\n    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\\n) -> np.ndarray:\\n    n = np.shape(np.arange(len_df - window))[0]\\n    A = np.empty((n, 2))\\n    for i in tqdm(np.arange(len_df - window)):\\n        tmp_avg = avg[i : i + window]\\n        tmp_v = v[i : i + window]\\n        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\\n        jj = idx[i + window]\\n        A[i, 0] = jj\\n        A[i, 1] = aa\\n    return A\\n\\n\\ndef add_rolling_vwap(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    v = df.volume.values\\n    avg = df[column].values\\n    idx = df.index.asi8\\n    # A = numba_vwap(avg, v, idx, len(df), window)\\n    A = numpy_vwap(avg, v, idx, len(df), window)\\n    outdf = (\\n        pd.DataFrame(A, columns=[\\\"index\\\", f\\\"rvwap_{window}\\\"])\\n        .assign(datetime=lambda df: pd.to_datetime(df[\\\"index\\\"], unit=\\\"ns\\\"))\\n        .drop(\\\"index\\\", axis=1)\\n        .set_index(\\\"datetime\\\")\\n    )\\n    df = df.join(outdf, how=\\\"left\\\")\\n    return df\\n\\n\\n#################\\ndef add_rolling_bands(\\n    df: pd.DataFrame, column: str, dist: int, window: int\\n) -> pd.DataFrame:\\n    upper = df[column] + dist * df[column].rolling(window).std()\\n    lower = df[column] - dist * df[column].rolling(window).std()\\n\\n    df[f\\\"upper_band_{column}\\\"] = upper\\n    df[f\\\"lower_band_{column}\\\"] = lower\\n    return df\\n\\n\\n#################\\ndef add_acceleration(\\n    df: pd.DataFrame, column: str = \\\"close\\\", window: int = 10\\n) -> pd.DataFrame:\\n    return_diff = df[column].pct_change().diff()\\n    df[f\\\"racc_{column}_{window}\\\"] = return_diff.rolling(\\n        window\\n    ).std()  # standard deviation of second deriv aka acceleration\\n    return df\\n\\n\\ndef roll_rank_bk(array):\\n    rank = array.size + 1 - bk.rankdata(array)[-1]\\n    A = array.shape[0]\\n    p = rank / A\\n    return p\\n\\n\\n#################\\ndef add_volatility(\\n    df: pd.DataFrame, column: str = \\\"close\\\", window: int = 10\\n) -> pd.DataFrame:\\n    returns = df[column].pct_change()\\n    df[f\\\"rvol_{column}_{window}\\\"] = returns.rolling(window).std()\\n    return df\\n\\n\\ndef relative_strength_index(df: pd.DataFrame, n: int) -> pd.Series:\\n    \\\"\\\"\\\"\\n    Calculate Relative Strength Index(RSI) for given data.\\n    https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\\n\\n    :param df: pandas.DataFrame\\n    :param n:\\n    :return: pandas.DataFrame\\n    \\\"\\\"\\\"\\n    i = 0\\n    UpI = [0]\\n    DoI = [0]\\n    while i + 1 <= df.index[-1]:\\n        UpMove = df.loc[i + 1, \\\"high\\\"] - df.loc[i, \\\"high\\\"]\\n        DoMove = df.loc[i, \\\"low\\\"] - df.loc[i + 1, \\\"low\\\"]\\n        if UpMove > DoMove and UpMove > 0:\\n            UpD = UpMove\\n        else:\\n            UpD = 0\\n        UpI.append(UpD)\\n        if DoMove > UpMove and DoMove > 0:\\n            DoD = DoMove\\n        else:\\n            DoD = 0\\n        DoI.append(DoD)\\n        i = i + 1\\n    UpI = pd.Series(UpI)\\n    DoI = pd.Series(DoI)\\n    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\\n    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\\n    RSI = pd.Series(round(PosDI * 100.0 / (PosDI + NegDI)), name=\\\"RSI_\\\" + str(n))\\n    return RSI\\n\\n\\ndef add_rsi(df: pd.DataFrame, column: str = \\\"close\\\", window: int = 14) -> pd.DataFrame:\\n    out = df.reset_index()\\n    rsi = relative_strength_index(out, window)\\n    df[f\\\"rsi_{column}_{window}\\\"] = pd.Series(data=rsi.values, index=df.index)\\n    return df\\n\\n\\n#################\\n\\n\\ndef np_racorr(array: np.ndarray, window: int, lag: int) -> np.ndarray:\\n    \\\"\\\"\\\"\\n    rolling autocorrelation\\n    \\\"\\\"\\\"\\n    return npx.rolling_apply(\\n        lambda array, lag: sm.tsa.acf(array, nlags=lag, fft=True)[lag],\\n        window,\\n        array,\\n        lag=lag,\\n        n_jobs=10,\\n    )\\n\\n\\ndef add_rolling_autocorr(\\n    df: pd.DataFrame, column: str, window: int, lag: int\\n) -> pd.DataFrame:\\n    log_changes_array = np.log(df[column]).diff().values\\n    df[f\\\"racorr_{column}_{window}\\\"] = np_racorr(log_changes_array, window, lag)\\n    return df\\n\\n\\n#################\\n@nb.njit\\ndef custom_percentile(array: np.ndarray) -> float:\\n    if (array.shape[0] - 1) == 0:\\n        return np.nan\\n    return (array[:-1] > array[-1]).sum() / (array.shape[0] - 1)\\n\\n\\ndef add_custom_percentile(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"rank_{column}_{window}\\\"] = npx.rolling_apply(\\n        custom_percentile, window, df[column].values, n_jobs=5\\n    )\\n    return df\";\n",
       "                var nbb_formatted_code = \"#################\\n\\n\\ndef add_log_returns(df: pd.DataFrame, column: str) -> pd.DataFrame:\\n    df[f\\\"log_{column}_return\\\"] = np.log(df[column]).diff()\\n    return df\\n\\n\\n#################\\ndef internal_bar_strength(df: pd.DataFrame) -> float:\\n    return (df.close - df.low) / (df.high - df.low)\\n\\n\\ndef add_internal_bar_strength(df: pd.DataFrame) -> pd.DataFrame:\\n    df[\\\"ibs\\\"] = internal_bar_strength(df)\\n    return df\\n\\n\\n#################\\n# @nb.jit\\ndef aqr_momentum(array: np.ndarray) -> float:\\n    returns = np.diff(np.log(array))  # .diff()\\n    x = np.arange(len(returns))\\n    slope, _, rvalue, _, _ = stats.linregress(x, returns)\\n    return ((1 + slope) ** 252) * (rvalue ** 2)  # annualize slope and multiply by R^2\\n\\n\\n@nb.njit\\ndef aqr_momo_numba(array: np.ndarray) -> float:\\n    y = np.diff(np.log(array))\\n    x = np.arange(y.shape[0])\\n    A = np.column_stack((x, np.ones(x.shape[0])))\\n    model, resid = np.linalg.lstsq(A, y)[:2]\\n    r2 = 1 - resid / (y.size * y.var())\\n    return (((1 + model[0]) ** 252) * r2)[0]\\n\\n\\ndef add_aqr_momentum(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"aqr_momo_{column}_{window}\\\"] = npx.rolling_apply(\\n        aqr_momentum, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\ndef add_aqr_momentum_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"aqr_momo_{column}_{window}\\\"] = npx.rolling_apply(\\n        aqr_momo_numba, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\n#################\\n\\n\\ndef get_slope(array: np.ndarray) -> float:\\n    returns = np.diff(np.log(array))\\n    x = np.arange(len(returns))\\n    slope, _, rvalue, _, _ = stats.linregress(x, returns)\\n    return slope\\n\\n\\n@nb.njit\\ndef get_slope_numba(array: np.ndarray) -> float:\\n    y = np.diff(np.log(array))\\n    # y = y[~np.isnan(y)]\\n    x = np.arange(y.shape[0])\\n    A = np.column_stack((x, np.ones(x.shape[0])))\\n    model, resid = np.linalg.lstsq(A, y)[:2]\\n    return model[0]\\n\\n\\ndef add_slope_column(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"slope_{column}_{window}\\\"] = npx.rolling_apply(\\n        get_slope, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\ndef add_slope_column_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"slope_{column}_{window}\\\"] = npx.rolling_apply(\\n        get_slope_numba, window, df[column].values, n_jobs=1\\n    )\\n    return df\\n\\n\\n#################\\ndef add_average_price(df: pd.DataFrame) -> pd.DataFrame:\\n    df[\\\"average_price\\\"] = (df.high + df.low + df.close + df.open) / 4\\n    return df\\n\\n\\n#################\\ndef add_rolling_min(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    array = npx.rolling_apply(np.min, window, df[column].values, n_jobs=10)\\n    df[f\\\"rmin_{column}_{window}\\\"] = array\\n    return df\\n\\n\\ndef add_rolling_max(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    array = npx.rolling_apply(np.max, window, df[column].values, n_jobs=10)\\n    df[f\\\"rmax_{column}_{window}\\\"] = array\\n    return df\\n\\n\\n#################\\n\\n# for some reason njit is generating zerodivision errors whereas numpy is not\\n@nb.njit\\ndef numba_vwap(\\n    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\\n) -> np.ndarray:\\n    n = np.shape(np.arange(len_df - window))[0]\\n    A = np.empty((n, 2))\\n    for i in np.arange(len_df - window):\\n        tmp_avg = avg[i : i + window]\\n        tmp_v = v[i : i + window]\\n        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\\n        jj = idx[i + window]\\n        A[i, 0] = jj\\n        A[i, 1] = aa\\n    return A\\n\\n\\ndef numpy_vwap(\\n    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\\n) -> np.ndarray:\\n    n = np.shape(np.arange(len_df - window))[0]\\n    A = np.empty((n, 2))\\n    for i in tqdm(np.arange(len_df - window)):\\n        tmp_avg = avg[i : i + window]\\n        tmp_v = v[i : i + window]\\n        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\\n        jj = idx[i + window]\\n        A[i, 0] = jj\\n        A[i, 1] = aa\\n    return A\\n\\n\\ndef add_rolling_vwap(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    v = df.volume.values\\n    avg = df[column].values\\n    idx = df.index.asi8\\n    # A = numba_vwap(avg, v, idx, len(df), window)\\n    A = numpy_vwap(avg, v, idx, len(df), window)\\n    outdf = (\\n        pd.DataFrame(A, columns=[\\\"index\\\", f\\\"rvwap_{window}\\\"])\\n        .assign(datetime=lambda df: pd.to_datetime(df[\\\"index\\\"], unit=\\\"ns\\\"))\\n        .drop(\\\"index\\\", axis=1)\\n        .set_index(\\\"datetime\\\")\\n    )\\n    df = df.join(outdf, how=\\\"left\\\")\\n    return df\\n\\n\\n#################\\ndef add_rolling_bands(\\n    df: pd.DataFrame, column: str, dist: int, window: int\\n) -> pd.DataFrame:\\n    upper = df[column] + dist * df[column].rolling(window).std()\\n    lower = df[column] - dist * df[column].rolling(window).std()\\n\\n    df[f\\\"upper_band_{column}\\\"] = upper\\n    df[f\\\"lower_band_{column}\\\"] = lower\\n    return df\\n\\n\\n#################\\ndef add_acceleration(\\n    df: pd.DataFrame, column: str = \\\"close\\\", window: int = 10\\n) -> pd.DataFrame:\\n    return_diff = df[column].pct_change().diff()\\n    df[f\\\"racc_{column}_{window}\\\"] = return_diff.rolling(\\n        window\\n    ).std()  # standard deviation of second deriv aka acceleration\\n    return df\\n\\n\\ndef roll_rank_bk(array):\\n    rank = array.size + 1 - bk.rankdata(array)[-1]\\n    A = array.shape[0]\\n    p = rank / A\\n    return p\\n\\n\\n#################\\ndef add_volatility(\\n    df: pd.DataFrame, column: str = \\\"close\\\", window: int = 10\\n) -> pd.DataFrame:\\n    returns = df[column].pct_change()\\n    df[f\\\"rvol_{column}_{window}\\\"] = returns.rolling(window).std()\\n    return df\\n\\n\\ndef relative_strength_index(df: pd.DataFrame, n: int) -> pd.Series:\\n    \\\"\\\"\\\"\\n    Calculate Relative Strength Index(RSI) for given data.\\n    https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\\n\\n    :param df: pandas.DataFrame\\n    :param n:\\n    :return: pandas.DataFrame\\n    \\\"\\\"\\\"\\n    i = 0\\n    UpI = [0]\\n    DoI = [0]\\n    while i + 1 <= df.index[-1]:\\n        UpMove = df.loc[i + 1, \\\"high\\\"] - df.loc[i, \\\"high\\\"]\\n        DoMove = df.loc[i, \\\"low\\\"] - df.loc[i + 1, \\\"low\\\"]\\n        if UpMove > DoMove and UpMove > 0:\\n            UpD = UpMove\\n        else:\\n            UpD = 0\\n        UpI.append(UpD)\\n        if DoMove > UpMove and DoMove > 0:\\n            DoD = DoMove\\n        else:\\n            DoD = 0\\n        DoI.append(DoD)\\n        i = i + 1\\n    UpI = pd.Series(UpI)\\n    DoI = pd.Series(DoI)\\n    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\\n    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\\n    RSI = pd.Series(round(PosDI * 100.0 / (PosDI + NegDI)), name=\\\"RSI_\\\" + str(n))\\n    return RSI\\n\\n\\ndef add_rsi(df: pd.DataFrame, column: str = \\\"close\\\", window: int = 14) -> pd.DataFrame:\\n    out = df.reset_index()\\n    rsi = relative_strength_index(out, window)\\n    df[f\\\"rsi_{column}_{window}\\\"] = pd.Series(data=rsi.values, index=df.index)\\n    return df\\n\\n\\n#################\\n\\n\\ndef np_racorr(array: np.ndarray, window: int, lag: int) -> np.ndarray:\\n    \\\"\\\"\\\"\\n    rolling autocorrelation\\n    \\\"\\\"\\\"\\n    return npx.rolling_apply(\\n        lambda array, lag: sm.tsa.acf(array, nlags=lag, fft=True)[lag],\\n        window,\\n        array,\\n        lag=lag,\\n        n_jobs=10,\\n    )\\n\\n\\ndef add_rolling_autocorr(\\n    df: pd.DataFrame, column: str, window: int, lag: int\\n) -> pd.DataFrame:\\n    log_changes_array = np.log(df[column]).diff().values\\n    df[f\\\"racorr_{column}_{window}\\\"] = np_racorr(log_changes_array, window, lag)\\n    return df\\n\\n\\n#################\\n@nb.njit\\ndef custom_percentile(array: np.ndarray) -> float:\\n    if (array.shape[0] - 1) == 0:\\n        return np.nan\\n    return (array[:-1] > array[-1]).sum() / (array.shape[0] - 1)\\n\\n\\ndef add_custom_percentile(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"rank_{column}_{window}\\\"] = npx.rolling_apply(\\n        custom_percentile, window, df[column].values, n_jobs=5\\n    )\\n    return df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################\n",
    "\n",
    "\n",
    "def add_log_returns(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    df[f\"log_{column}_return\"] = np.log(df[column]).diff()\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def internal_bar_strength(df: pd.DataFrame) -> float:\n",
    "    return (df.close - df.low) / (df.high - df.low)\n",
    "\n",
    "\n",
    "def add_internal_bar_strength(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"ibs\"] = internal_bar_strength(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "# @nb.jit\n",
    "def aqr_momentum(array: np.ndarray) -> float:\n",
    "    returns = np.diff(np.log(array))  # .diff()\n",
    "    x = np.arange(len(returns))\n",
    "    slope, _, rvalue, _, _ = stats.linregress(x, returns)\n",
    "    return ((1 + slope) ** 252) * (rvalue ** 2)  # annualize slope and multiply by R^2\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def aqr_momo_numba(array: np.ndarray) -> float:\n",
    "    y = np.diff(np.log(array))\n",
    "    x = np.arange(y.shape[0])\n",
    "    A = np.column_stack((x, np.ones(x.shape[0])))\n",
    "    model, resid = np.linalg.lstsq(A, y)[:2]\n",
    "    r2 = 1 - resid / (y.size * y.var())\n",
    "    return (((1 + model[0]) ** 252) * r2)[0]\n",
    "\n",
    "\n",
    "def add_aqr_momentum(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"aqr_momo_{column}_{window}\"] = npx.rolling_apply(\n",
    "        aqr_momentum, window, df[column].values, n_jobs=10\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_aqr_momentum_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"aqr_momo_{column}_{window}\"] = npx.rolling_apply(\n",
    "        aqr_momo_numba, window, df[column].values, n_jobs=10\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "def get_slope(array: np.ndarray) -> float:\n",
    "    returns = np.diff(np.log(array))\n",
    "    x = np.arange(len(returns))\n",
    "    slope, _, rvalue, _, _ = stats.linregress(x, returns)\n",
    "    return slope\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def get_slope_numba(array: np.ndarray) -> float:\n",
    "    y = np.diff(np.log(array))\n",
    "    # y = y[~np.isnan(y)]\n",
    "    x = np.arange(y.shape[0])\n",
    "    A = np.column_stack((x, np.ones(x.shape[0])))\n",
    "    model, resid = np.linalg.lstsq(A, y)[:2]\n",
    "    return model[0]\n",
    "\n",
    "\n",
    "def add_slope_column(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"slope_{column}_{window}\"] = npx.rolling_apply(\n",
    "        get_slope, window, df[column].values, n_jobs=10\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_slope_column_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"slope_{column}_{window}\"] = npx.rolling_apply(\n",
    "        get_slope_numba, window, df[column].values, n_jobs=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_average_price(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"average_price\"] = (df.high + df.low + df.close + df.open) / 4\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_rolling_min(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    array = npx.rolling_apply(np.min, window, df[column].values, n_jobs=10)\n",
    "    df[f\"rmin_{column}_{window}\"] = array\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rolling_max(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    array = npx.rolling_apply(np.max, window, df[column].values, n_jobs=10)\n",
    "    df[f\"rmax_{column}_{window}\"] = array\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "# for some reason njit is generating zerodivision errors whereas numpy is not\n",
    "@nb.njit\n",
    "def numba_vwap(\n",
    "    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\n",
    ") -> np.ndarray:\n",
    "    n = np.shape(np.arange(len_df - window))[0]\n",
    "    A = np.empty((n, 2))\n",
    "    for i in np.arange(len_df - window):\n",
    "        tmp_avg = avg[i : i + window]\n",
    "        tmp_v = v[i : i + window]\n",
    "        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\n",
    "        jj = idx[i + window]\n",
    "        A[i, 0] = jj\n",
    "        A[i, 1] = aa\n",
    "    return A\n",
    "\n",
    "\n",
    "def numpy_vwap(\n",
    "    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\n",
    ") -> np.ndarray:\n",
    "    n = np.shape(np.arange(len_df - window))[0]\n",
    "    A = np.empty((n, 2))\n",
    "    for i in tqdm(np.arange(len_df - window)):\n",
    "        tmp_avg = avg[i : i + window]\n",
    "        tmp_v = v[i : i + window]\n",
    "        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\n",
    "        jj = idx[i + window]\n",
    "        A[i, 0] = jj\n",
    "        A[i, 1] = aa\n",
    "    return A\n",
    "\n",
    "\n",
    "def add_rolling_vwap(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    v = df.volume.values\n",
    "    avg = df[column].values\n",
    "    idx = df.index.asi8\n",
    "    # A = numba_vwap(avg, v, idx, len(df), window)\n",
    "    A = numpy_vwap(avg, v, idx, len(df), window)\n",
    "    outdf = (\n",
    "        pd.DataFrame(A, columns=[\"index\", f\"rvwap_{window}\"])\n",
    "        .assign(datetime=lambda df: pd.to_datetime(df[\"index\"], unit=\"ns\"))\n",
    "        .drop(\"index\", axis=1)\n",
    "        .set_index(\"datetime\")\n",
    "    )\n",
    "    df = df.join(outdf, how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_rolling_bands(\n",
    "    df: pd.DataFrame, column: str, dist: int, window: int\n",
    ") -> pd.DataFrame:\n",
    "    upper = df[column] + dist * df[column].rolling(window).std()\n",
    "    lower = df[column] - dist * df[column].rolling(window).std()\n",
    "\n",
    "    df[f\"upper_band_{column}\"] = upper\n",
    "    df[f\"lower_band_{column}\"] = lower\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_acceleration(\n",
    "    df: pd.DataFrame, column: str = \"close\", window: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    return_diff = df[column].pct_change().diff()\n",
    "    df[f\"racc_{column}_{window}\"] = return_diff.rolling(\n",
    "        window\n",
    "    ).std()  # standard deviation of second deriv aka acceleration\n",
    "    return df\n",
    "\n",
    "\n",
    "def roll_rank_bk(array):\n",
    "    rank = array.size + 1 - bk.rankdata(array)[-1]\n",
    "    A = array.shape[0]\n",
    "    p = rank / A\n",
    "    return p\n",
    "\n",
    "\n",
    "#################\n",
    "def add_volatility(\n",
    "    df: pd.DataFrame, column: str = \"close\", window: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    returns = df[column].pct_change()\n",
    "    df[f\"rvol_{column}_{window}\"] = returns.rolling(window).std()\n",
    "    return df\n",
    "\n",
    "\n",
    "def relative_strength_index(df: pd.DataFrame, n: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate Relative Strength Index(RSI) for given data.\n",
    "    https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\n",
    "\n",
    "    :param df: pandas.DataFrame\n",
    "    :param n:\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    UpI = [0]\n",
    "    DoI = [0]\n",
    "    while i + 1 <= df.index[-1]:\n",
    "        UpMove = df.loc[i + 1, \"high\"] - df.loc[i, \"high\"]\n",
    "        DoMove = df.loc[i, \"low\"] - df.loc[i + 1, \"low\"]\n",
    "        if UpMove > DoMove and UpMove > 0:\n",
    "            UpD = UpMove\n",
    "        else:\n",
    "            UpD = 0\n",
    "        UpI.append(UpD)\n",
    "        if DoMove > UpMove and DoMove > 0:\n",
    "            DoD = DoMove\n",
    "        else:\n",
    "            DoD = 0\n",
    "        DoI.append(DoD)\n",
    "        i = i + 1\n",
    "    UpI = pd.Series(UpI)\n",
    "    DoI = pd.Series(DoI)\n",
    "    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\n",
    "    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\n",
    "    RSI = pd.Series(round(PosDI * 100.0 / (PosDI + NegDI)), name=\"RSI_\" + str(n))\n",
    "    return RSI\n",
    "\n",
    "\n",
    "def add_rsi(df: pd.DataFrame, column: str = \"close\", window: int = 14) -> pd.DataFrame:\n",
    "    out = df.reset_index()\n",
    "    rsi = relative_strength_index(out, window)\n",
    "    df[f\"rsi_{column}_{window}\"] = pd.Series(data=rsi.values, index=df.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "def np_racorr(array: np.ndarray, window: int, lag: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    rolling autocorrelation\n",
    "    \"\"\"\n",
    "    return npx.rolling_apply(\n",
    "        lambda array, lag: sm.tsa.acf(array, nlags=lag, fft=True)[lag],\n",
    "        window,\n",
    "        array,\n",
    "        lag=lag,\n",
    "        n_jobs=10,\n",
    "    )\n",
    "\n",
    "\n",
    "def add_rolling_autocorr(\n",
    "    df: pd.DataFrame, column: str, window: int, lag: int\n",
    ") -> pd.DataFrame:\n",
    "    log_changes_array = np.log(df[column]).diff().values\n",
    "    df[f\"racorr_{column}_{window}\"] = np_racorr(log_changes_array, window, lag)\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "@nb.njit\n",
    "def custom_percentile(array: np.ndarray) -> float:\n",
    "    if (array.shape[0] - 1) == 0:\n",
    "        return np.nan\n",
    "    return (array[:-1] > array[-1]).sum() / (array.shape[0] - 1)\n",
    "\n",
    "\n",
    "def add_custom_percentile(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"rank_{column}_{window}\"] = npx.rolling_apply(\n",
    "        custom_percentile, window, df[column].values, n_jobs=5\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:04.242285Z",
     "start_time": "2021-03-29T23:51:03.749602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# create data store for features\\n\\nstore = pd.HDFStore(\\n    processed / \\\"spy_features.h5\\\", mode=\\\"a\\\", complevel=1, complib=\\\"blosc:lz4\\\"\\n)\\nstore.close()\";\n",
       "                var nbb_formatted_code = \"# create data store for features\\n\\nstore = pd.HDFStore(\\n    processed / \\\"spy_features.h5\\\", mode=\\\"a\\\", complevel=1, complib=\\\"blosc:lz4\\\"\\n)\\nstore.close()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create data store for features\n",
    "\n",
    "store = pd.HDFStore(\n",
    "    processed / \"spy_features.h5\", mode=\"a\", complevel=1, complib=\"blosc:lz4\"\n",
    ")\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:04.537852Z",
     "start_time": "2021-03-29T23:51:04.246275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"from pprint import pprint\";\n",
       "                var nbb_formatted_code = \"from pprint import pprint\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 min multi day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:42:32.160209Z",
     "start_time": "2021-03-29T23:51:04.541841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_day': 2880,\n",
      " '1_day': 288,\n",
      " '21_day': 6048,\n",
      " '2_day': 576,\n",
      " '3_day': 864,\n",
      " '5_day': 1440}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 297285/297285 [00:10<00:00, 28894.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [04:57<00:00, 11.45s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 296997/296997 [00:07<00:00, 37962.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [04:45<00:00, 10.99s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 296709/296709 [00:04<00:00, 59917.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [04:09<00:00,  9.59s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 296133/296133 [00:05<00:00, 54950.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [04:16<00:00,  9.87s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 294693/294693 [00:06<00:00, 46388.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [04:26<00:00, 10.25s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 291525/291525 [00:09<00:00, 31028.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [04:14<00:00,  9.79s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [51:27<00:00, 514.52s/it]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 5)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\"]\\nperiods = dict(\\n    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"5m\\\"\\ndf = data[data_frequency].copy()\\n\\nfor key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(tmp_df, column, window=int(window // 10))\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/5m/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_formatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 5)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\"]\\nperiods = dict(\\n    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"5m\\\"\\ndf = data[data_frequency].copy()\\n\\nfor key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(tmp_df, column, window=int(window // 10))\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/5m/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_day_in_minutes = 1440\n",
    "one_day = int(one_day_in_minutes // 5)\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "\n",
    "period_labels = [\"1_day\", \"2_day\", \"3_day\", \"5_day\", \"10_day\", \"21_day\"]\n",
    "periods = dict(\n",
    "    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\n",
    ")\n",
    "log_errors = []\n",
    "pprint(periods)\n",
    "\n",
    "data_frequency = \"5m\"\n",
    "df = data[data_frequency].copy()\n",
    "\n",
    "for key, window in tqdm(periods.items()):\n",
    "    tqdm._instances.clear()\n",
    "    try:\n",
    "        tmp_df = (\n",
    "            df.pipe(add_average_price)\n",
    "            .pipe(add_rolling_vwap, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_bands, column=f\"rvwap_{window}\", dist=2, window=window)\n",
    "            .pipe(add_internal_bar_strength)\n",
    "            .pipe(add_rolling_min, column=\"low\", window=window)\n",
    "            .pipe(add_rolling_max, column=\"high\", window=window)\n",
    "            .dropna()\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"lower_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"upper_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(add_slope_column_numba, column=f\"rmin_low_{window}\", window=window)\n",
    "            .pipe(add_slope_column_numba, column=f\"rmax_high_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"average_price\", window=window)\n",
    "            .pipe(add_acceleration, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_volatility, column=\"close\", window=window)\n",
    "            .pipe(add_volatility, column=\"average_price\", window=window)\n",
    "            .pipe(add_rsi, column=\"close\", window=window)\n",
    "            .pipe(add_rsi, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_autocorr, column=\"close\", window=window, lag=1)\n",
    "            .pipe(add_rolling_autocorr, column=\"average_price\", window=window, lag=1)\n",
    "        )\n",
    "        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\"up\") :]\n",
    "        for column in tqdm(columns_to_rank):\n",
    "            tmp_df = add_custom_percentile(tmp_df, column, window=int(window // 10))\n",
    "            # ^^ make window smaller since this basically a double lag\n",
    "\n",
    "        # write to store iteratively in case of problems\n",
    "        with pd.HDFStore(processed / \"spy_features.h5\") as store:\n",
    "            store.put(value=tmp_df, key=f\"spy/5m/{key}\", format=\"table\")\n",
    "\n",
    "    except Exception as error:\n",
    "        log_error = dict(window=window, error=error)\n",
    "        log_errors.append(log_error)\n",
    "        pprint(log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 min multi day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:42:32.550118Z",
     "start_time": "2021-03-30T00:42:32.165240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"from pprint import pprint\";\n",
       "                var nbb_formatted_code = \"from pprint import pprint\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:42:32.781498Z",
     "start_time": "2021-03-30T00:42:32.553109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_day': 480,\n",
      " '1_day': 48,\n",
      " '21_day': 1008,\n",
      " '2_day': 96,\n",
      " '3_day': 144,\n",
      " '5_day': 240}\n",
      "-------------------------------------------------------------------------------\n",
      "dataframe information\n",
      "-------------------------------------------------------------------------------\n",
      "                       open    high     low   close       up     down   volume\n",
      "datetime                                                                      \n",
      "2019-05-14 11:00:00  284.90  285.05  284.30  285.03  1982877  1488027  3470904\n",
      "2019-05-14 11:30:00  285.03  285.05  284.03  284.32  1803471  2137726  3941197\n",
      "2019-05-14 12:00:00  284.32  284.77  284.14  284.49  1676140  1627154  3303294\n",
      "2019-05-14 12:30:00  284.49  284.52  283.40  283.42  4500359  5440864  9941223\n",
      "2019-05-14 13:00:00  283.42  283.49  283.17  283.32   933738  1283851  2217589\n",
      "--------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 52754 entries, 2004-05-14 06:30:00 to 2019-05-14 13:00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   open    52754 non-null  float64\n",
      " 1   high    52754 non-null  float64\n",
      " 2   low     52754 non-null  float64\n",
      " 3   close   52754 non-null  float64\n",
      " 4   up      52754 non-null  int64  \n",
      " 5   down    52754 non-null  int64  \n",
      " 6   volume  52754 non-null  int64  \n",
      "dtypes: float64(4), int64(3)\n",
      "memory usage: 3.2 MB\n",
      "None\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 30)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\"]\\nperiods = dict(\\n    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"30m\\\"\\ndf = data[data_frequency].copy()\\ncprint(df)\";\n",
       "                var nbb_formatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 30)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\"]\\nperiods = dict(\\n    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"30m\\\"\\ndf = data[data_frequency].copy()\\ncprint(df)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_day_in_minutes = 1440\n",
    "one_day = int(one_day_in_minutes // 30)\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "\n",
    "period_labels = [\"1_day\", \"2_day\", \"3_day\", \"5_day\", \"10_day\", \"21_day\"]\n",
    "periods = dict(\n",
    "    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\n",
    ")\n",
    "log_errors = []\n",
    "pprint(periods)\n",
    "\n",
    "data_frequency = \"30m\"\n",
    "df = data[data_frequency].copy()\n",
    "cprint(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:52:17.329471Z",
     "start_time": "2021-03-30T00:42:32.783493Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 52706/52706 [00:00<00:00, 57143.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:54<00:00,  2.10s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 52658/52658 [00:00<00:00, 66394.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:55<00:00,  2.12s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 52610/52610 [00:00<00:00, 63735.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:54<00:00,  2.09s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 52514/52514 [00:00<00:00, 59397.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:55<00:00,  2.12s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 52274/52274 [00:00<00:00, 57459.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:53<00:00,  2.06s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 51746/51746 [00:01<00:00, 47835.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:52<00:00,  2.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [09:44<00:00, 97.39s/it]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"for key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(tmp_df, column, window=int(window // 10))\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_formatted_code = \"for key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(tmp_df, column, window=int(window // 10))\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key, window in tqdm(periods.items()):\n",
    "    tqdm._instances.clear()\n",
    "    try:\n",
    "        tmp_df = (\n",
    "            df.pipe(add_average_price)\n",
    "            .pipe(add_rolling_vwap, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_bands, column=f\"rvwap_{window}\", dist=2, window=window)\n",
    "            .pipe(add_internal_bar_strength)\n",
    "            .pipe(add_rolling_min, column=\"low\", window=window)\n",
    "            .pipe(add_rolling_max, column=\"high\", window=window)\n",
    "            .dropna()\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"lower_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"upper_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(add_slope_column_numba, column=f\"rmin_low_{window}\", window=window)\n",
    "            .pipe(add_slope_column_numba, column=f\"rmax_high_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"average_price\", window=window)\n",
    "            .pipe(add_acceleration, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_volatility, column=\"close\", window=window)\n",
    "            .pipe(add_volatility, column=\"average_price\", window=window)\n",
    "            .pipe(add_rsi, column=\"close\", window=window)\n",
    "            .pipe(add_rsi, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_autocorr, column=\"close\", window=window, lag=1)\n",
    "            .pipe(add_rolling_autocorr, column=\"average_price\", window=window, lag=1)\n",
    "        )\n",
    "        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\"up\") :]\n",
    "        for column in tqdm(columns_to_rank):\n",
    "            tmp_df = add_custom_percentile(tmp_df, column, window=int(window // 10))\n",
    "            # ^^ make window smaller since this basically a double lag\n",
    "\n",
    "        # write to store iteratively in case of problems\n",
    "        with pd.HDFStore(processed / \"spy_features.h5\") as store:\n",
    "            store.put(value=tmp_df, key=f\"spy/{data_frequency}/{key}\", format=\"table\")\n",
    "\n",
    "    except Exception as error:\n",
    "        log_error = dict(window=window, error=error)\n",
    "        log_errors.append(log_error)\n",
    "        pprint(log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Hour (60 min) multi day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:59:36.628731Z",
     "start_time": "2021-03-30T00:52:17.331466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_day': 240,\n",
      " '1_day': 24,\n",
      " '21_day': 504,\n",
      " '2_day': 48,\n",
      " '3_day': 72,\n",
      " '5_day': 120,\n",
      " '63_day': 1512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 30125/30125 [00:00<00:00, 63750.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:34<00:00,  1.34s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 30101/30101 [00:00<00:00, 68800.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:33<00:00,  1.30s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 30077/30077 [00:00<00:00, 65984.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:32<00:00,  1.25s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 30029/30029 [00:00<00:00, 65281.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:33<00:00,  1.28s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 29909/29909 [00:00<00:00, 58989.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:32<00:00,  1.24s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 29645/29645 [00:00<00:00, 59088.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:31<00:00,  1.23s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 28637/28637 [00:00<00:00, 52093.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:31<00:00,  1.21s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:19<00:00, 62.72s/it]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 60)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\nthree_month = int(one_month * 3)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\", \\\"63_day\\\"]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [one_day, two_days, three_days, five_days, ten_days, one_month, three_month],\\n    )\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"1H\\\"\\ndf = data[data_frequency].copy()\\n\\nfor key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(tmp_df, column, window=int(window // 10))\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_formatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 60)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\nthree_month = int(one_month * 3)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\", \\\"63_day\\\"]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [one_day, two_days, three_days, five_days, ten_days, one_month, three_month],\\n    )\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"1H\\\"\\ndf = data[data_frequency].copy()\\n\\nfor key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(tmp_df, column, window=int(window // 10))\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_day_in_minutes = 1440\n",
    "one_day = int(one_day_in_minutes // 60)\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "three_month = int(one_month * 3)\n",
    "\n",
    "period_labels = [\"1_day\", \"2_day\", \"3_day\", \"5_day\", \"10_day\", \"21_day\", \"63_day\"]\n",
    "periods = dict(\n",
    "    zip(\n",
    "        period_labels,\n",
    "        [one_day, two_days, three_days, five_days, ten_days, one_month, three_month],\n",
    "    )\n",
    ")\n",
    "log_errors = []\n",
    "pprint(periods)\n",
    "\n",
    "data_frequency = \"1H\"\n",
    "df = data[data_frequency].copy()\n",
    "\n",
    "for key, window in tqdm(periods.items()):\n",
    "    tqdm._instances.clear()\n",
    "    try:\n",
    "        tmp_df = (\n",
    "            df.pipe(add_average_price)\n",
    "            .pipe(add_rolling_vwap, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_bands, column=f\"rvwap_{window}\", dist=2, window=window)\n",
    "            .pipe(add_internal_bar_strength)\n",
    "            .pipe(add_rolling_min, column=\"low\", window=window)\n",
    "            .pipe(add_rolling_max, column=\"high\", window=window)\n",
    "            .dropna()\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"lower_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"upper_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(add_slope_column_numba, column=f\"rmin_low_{window}\", window=window)\n",
    "            .pipe(add_slope_column_numba, column=f\"rmax_high_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"average_price\", window=window)\n",
    "            .pipe(add_acceleration, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_volatility, column=\"close\", window=window)\n",
    "            .pipe(add_volatility, column=\"average_price\", window=window)\n",
    "            .pipe(add_rsi, column=\"close\", window=window)\n",
    "            .pipe(add_rsi, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_autocorr, column=\"close\", window=window, lag=1)\n",
    "            .pipe(add_rolling_autocorr, column=\"average_price\", window=window, lag=1)\n",
    "        )\n",
    "        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\"up\") :]\n",
    "        for column in tqdm(columns_to_rank):\n",
    "            tmp_df = add_custom_percentile(tmp_df, column, window=int(window // 10))\n",
    "            # ^^ make window smaller since this basically a double lag\n",
    "\n",
    "        # write to store iteratively in case of problems\n",
    "        with pd.HDFStore(processed / \"spy_features.h5\") as store:\n",
    "            store.put(value=tmp_df, key=f\"spy/{data_frequency}/{key}\", format=\"table\")\n",
    "\n",
    "    except Exception as error:\n",
    "        log_error = dict(window=window, error=error)\n",
    "        log_errors.append(log_error)\n",
    "        pprint(log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T01:03:19.895463Z",
     "start_time": "2021-03-30T00:59:36.630727Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_day': 10,\n",
      " '126_day': 126,\n",
      " '21_day': 21,\n",
      " '252_day': 252,\n",
      " '2_day': 2,\n",
      " '3_day': 3,\n",
      " '5_day': 5,\n",
      " '63_day': 63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 3772/3772 [00:00<00:00, 56449.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:10<00:00,  2.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3771/3771 [00:00<00:00, 68694.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:10<00:00,  2.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3769/3769 [00:00<00:00, 59090.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:10<00:00,  2.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3764/3764 [00:00<00:00, 58240.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:11<00:00,  2.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3753/3753 [00:00<00:00, 60068.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:10<00:00,  2.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3711/3711 [00:00<00:00, 68907.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:10<00:00,  2.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3648/3648 [00:00<00:00, 74528.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:10<00:00,  2.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3522/3522 [00:00<00:00, 61874.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:09<00:00,  2.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [03:43<00:00, 27.88s/it]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"one_day = 1\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\nthree_month = int(one_month * 3)\\nsix_month = int(one_month * 6)\\ntwelve_month = int(one_month * 12)\\n\\nperiod_labels = [  # \\\"1_day\\\",\\n    \\\"2_day\\\",\\n    \\\"3_day\\\",\\n    \\\"5_day\\\",\\n    \\\"10_day\\\",\\n    \\\"21_day\\\",\\n    \\\"63_day\\\",\\n    \\\"126_day\\\",\\n    \\\"252_day\\\",\\n]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [  # one_day,\\n            two_days,\\n            three_days,\\n            five_days,\\n            ten_days,\\n            one_month,\\n            three_month,\\n            six_month,\\n            twelve_month,\\n        ],\\n    )\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"1D\\\"\\ndf = data[data_frequency].copy()\\n\\nfor key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(\\n                tmp_df, column, window=max(int(window // 2), 2)\\n            )\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_formatted_code = \"one_day = 1\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\nthree_month = int(one_month * 3)\\nsix_month = int(one_month * 6)\\ntwelve_month = int(one_month * 12)\\n\\nperiod_labels = [  # \\\"1_day\\\",\\n    \\\"2_day\\\",\\n    \\\"3_day\\\",\\n    \\\"5_day\\\",\\n    \\\"10_day\\\",\\n    \\\"21_day\\\",\\n    \\\"63_day\\\",\\n    \\\"126_day\\\",\\n    \\\"252_day\\\",\\n]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [  # one_day,\\n            two_days,\\n            three_days,\\n            five_days,\\n            ten_days,\\n            one_month,\\n            three_month,\\n            six_month,\\n            twelve_month,\\n        ],\\n    )\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"1D\\\"\\ndf = data[data_frequency].copy()\\n\\nfor key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(\\n                tmp_df, column, window=max(int(window // 2), 2)\\n            )\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_day = 1\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "three_month = int(one_month * 3)\n",
    "six_month = int(one_month * 6)\n",
    "twelve_month = int(one_month * 12)\n",
    "\n",
    "period_labels = [  # \"1_day\",\n",
    "    \"2_day\",\n",
    "    \"3_day\",\n",
    "    \"5_day\",\n",
    "    \"10_day\",\n",
    "    \"21_day\",\n",
    "    \"63_day\",\n",
    "    \"126_day\",\n",
    "    \"252_day\",\n",
    "]\n",
    "periods = dict(\n",
    "    zip(\n",
    "        period_labels,\n",
    "        [  # one_day,\n",
    "            two_days,\n",
    "            three_days,\n",
    "            five_days,\n",
    "            ten_days,\n",
    "            one_month,\n",
    "            three_month,\n",
    "            six_month,\n",
    "            twelve_month,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "log_errors = []\n",
    "pprint(periods)\n",
    "\n",
    "data_frequency = \"1D\"\n",
    "df = data[data_frequency].copy()\n",
    "\n",
    "for key, window in tqdm(periods.items()):\n",
    "    tqdm._instances.clear()\n",
    "    try:\n",
    "        tmp_df = (\n",
    "            df.pipe(add_average_price)\n",
    "            .pipe(add_rolling_vwap, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_bands, column=f\"rvwap_{window}\", dist=2, window=window)\n",
    "            .pipe(add_internal_bar_strength)\n",
    "            .pipe(add_rolling_min, column=\"low\", window=window)\n",
    "            .pipe(add_rolling_max, column=\"high\", window=window)\n",
    "            .dropna()\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"lower_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"upper_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(add_slope_column_numba, column=f\"rmin_low_{window}\", window=window)\n",
    "            .pipe(add_slope_column_numba, column=f\"rmax_high_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"average_price\", window=window)\n",
    "            .pipe(add_acceleration, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_volatility, column=\"close\", window=window)\n",
    "            .pipe(add_volatility, column=\"average_price\", window=window)\n",
    "            .pipe(add_rsi, column=\"close\", window=window)\n",
    "            .pipe(add_rsi, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_autocorr, column=\"close\", window=window, lag=1)\n",
    "            .pipe(add_rolling_autocorr, column=\"average_price\", window=window, lag=1)\n",
    "        )\n",
    "        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\"up\") :]\n",
    "        for column in tqdm(columns_to_rank):\n",
    "            tmp_df = add_custom_percentile(\n",
    "                tmp_df, column, window=max(int(window // 2), 2)\n",
    "            )\n",
    "            # ^^ make window smaller since this basically a double lag\n",
    "\n",
    "        # write to store iteratively in case of problems\n",
    "        with pd.HDFStore(processed / \"spy_features.h5\") as store:\n",
    "            store.put(value=tmp_df, key=f\"spy/{data_frequency}/{key}\", format=\"table\")\n",
    "\n",
    "    except Exception as error:\n",
    "        log_error = dict(window=window, error=error)\n",
    "        log_errors.append(log_error)\n",
    "        pprint(log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weekly features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T01:05:39.432425Z",
     "start_time": "2021-03-30T01:03:19.897408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'12_month': 48,\n",
      " '1_month': 4,\n",
      " '2_week': 2,\n",
      " '3_month': 12,\n",
      " '3_week': 3,\n",
      " '6_month': 24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 781/781 [00:00<00:00, 39157.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:08<00:00,  3.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 780/780 [00:00<00:00, 77847.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:08<00:00,  3.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 779/779 [00:00<00:00, 64904.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:08<00:00,  3.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 771/771 [00:00<00:00, 51537.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:07<00:00,  3.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 759/759 [00:00<00:00, 58340.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:08<00:00,  3.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 735/735 [00:00<00:00, 66952.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:06<00:00,  4.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [02:19<00:00, 23.21s/it]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"one_week = 1\\ntwo_weeks = int(one_week * 2)\\nthree_weeks = int(one_week * 3)\\none_month = int(one_week * 4)\\nthree_month = int(one_month * 3)\\nsix_month = int(one_month * 6)\\ntwelve_month = int(one_month * 12)\\n\\nperiod_labels = [\\n    # \\\"1_week\\\",\\n    \\\"2_week\\\",\\n    \\\"3_week\\\",\\n    \\\"1_month\\\",\\n    \\\"3_month\\\",\\n    \\\"6_month\\\",\\n    \\\"12_month\\\",\\n]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [\\n            # one_week,\\n            two_weeks,\\n            three_weeks,\\n            one_month,\\n            three_month,\\n            six_month,\\n            twelve_month,\\n        ],\\n    )\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"1W\\\"\\ndf = data[data_frequency].copy()\\n\\nfor key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(\\n                tmp_df, column, window=max(int(window // 2), 2)\\n            )\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_formatted_code = \"one_week = 1\\ntwo_weeks = int(one_week * 2)\\nthree_weeks = int(one_week * 3)\\none_month = int(one_week * 4)\\nthree_month = int(one_month * 3)\\nsix_month = int(one_month * 6)\\ntwelve_month = int(one_month * 12)\\n\\nperiod_labels = [\\n    # \\\"1_week\\\",\\n    \\\"2_week\\\",\\n    \\\"3_week\\\",\\n    \\\"1_month\\\",\\n    \\\"3_month\\\",\\n    \\\"6_month\\\",\\n    \\\"12_month\\\",\\n]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [\\n            # one_week,\\n            two_weeks,\\n            three_weeks,\\n            one_month,\\n            three_month,\\n            six_month,\\n            twelve_month,\\n        ],\\n    )\\n)\\nlog_errors = []\\npprint(periods)\\n\\ndata_frequency = \\\"1W\\\"\\ndf = data[data_frequency].copy()\\n\\nfor key, window in tqdm(periods.items()):\\n    tqdm._instances.clear()\\n    try:\\n        tmp_df = (\\n            df.pipe(add_average_price)\\n            .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window)\\n            .pipe(add_internal_bar_strength)\\n            .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n            .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n            .dropna()\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"lower_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(\\n                add_slope_column_numba,\\n                column=f\\\"upper_band_rvwap_{window}\\\",\\n                window=window,\\n            )\\n            .pipe(add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window)\\n            .pipe(add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n            .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n            .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n            .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n            .pipe(add_volatility, column=\\\"close\\\", window=window)\\n            .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rsi, column=\\\"close\\\", window=window)\\n            .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n            .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n            .pipe(add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1)\\n        )\\n        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n        for column in tqdm(columns_to_rank):\\n            tmp_df = add_custom_percentile(\\n                tmp_df, column, window=max(int(window // 2), 2)\\n            )\\n            # ^^ make window smaller since this basically a double lag\\n\\n        # write to store iteratively in case of problems\\n        with pd.HDFStore(processed / \\\"spy_features.h5\\\") as store:\\n            store.put(value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\")\\n\\n    except Exception as error:\\n        log_error = dict(window=window, error=error)\\n        log_errors.append(log_error)\\n        pprint(log_error)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_week = 1\n",
    "two_weeks = int(one_week * 2)\n",
    "three_weeks = int(one_week * 3)\n",
    "one_month = int(one_week * 4)\n",
    "three_month = int(one_month * 3)\n",
    "six_month = int(one_month * 6)\n",
    "twelve_month = int(one_month * 12)\n",
    "\n",
    "period_labels = [\n",
    "    # \"1_week\",\n",
    "    \"2_week\",\n",
    "    \"3_week\",\n",
    "    \"1_month\",\n",
    "    \"3_month\",\n",
    "    \"6_month\",\n",
    "    \"12_month\",\n",
    "]\n",
    "periods = dict(\n",
    "    zip(\n",
    "        period_labels,\n",
    "        [\n",
    "            # one_week,\n",
    "            two_weeks,\n",
    "            three_weeks,\n",
    "            one_month,\n",
    "            three_month,\n",
    "            six_month,\n",
    "            twelve_month,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "log_errors = []\n",
    "pprint(periods)\n",
    "\n",
    "data_frequency = \"1W\"\n",
    "df = data[data_frequency].copy()\n",
    "\n",
    "for key, window in tqdm(periods.items()):\n",
    "    tqdm._instances.clear()\n",
    "    try:\n",
    "        tmp_df = (\n",
    "            df.pipe(add_average_price)\n",
    "            .pipe(add_rolling_vwap, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_bands, column=f\"rvwap_{window}\", dist=2, window=window)\n",
    "            .pipe(add_internal_bar_strength)\n",
    "            .pipe(add_rolling_min, column=\"low\", window=window)\n",
    "            .pipe(add_rolling_max, column=\"high\", window=window)\n",
    "            .dropna()\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"lower_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(\n",
    "                add_slope_column_numba,\n",
    "                column=f\"upper_band_rvwap_{window}\",\n",
    "                window=window,\n",
    "            )\n",
    "            .pipe(add_slope_column_numba, column=f\"rmin_low_{window}\", window=window)\n",
    "            .pipe(add_slope_column_numba, column=f\"rmax_high_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=\"average_price\", window=window)\n",
    "            .pipe(add_acceleration, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_acceleration, column=\"close\", window=window)\n",
    "            .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "            .pipe(add_aqr_momentum_numba, column=f\"rvwap_{window}\", window=window)\n",
    "            .pipe(add_volatility, column=\"close\", window=window)\n",
    "            .pipe(add_volatility, column=\"average_price\", window=window)\n",
    "            .pipe(add_rsi, column=\"close\", window=window)\n",
    "            .pipe(add_rsi, column=\"average_price\", window=window)\n",
    "            .pipe(add_rolling_autocorr, column=\"close\", window=window, lag=1)\n",
    "            .pipe(add_rolling_autocorr, column=\"average_price\", window=window, lag=1)\n",
    "        )\n",
    "        columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\"up\") :]\n",
    "        for column in tqdm(columns_to_rank):\n",
    "            tmp_df = add_custom_percentile(\n",
    "                tmp_df, column, window=max(int(window // 2), 2)\n",
    "            )\n",
    "            # ^^ make window smaller since this basically a double lag\n",
    "\n",
    "        # write to store iteratively in case of problems\n",
    "        with pd.HDFStore(processed / \"spy_features.h5\") as store:\n",
    "            store.put(value=tmp_df, key=f\"spy/{data_frequency}/{key}\", format=\"table\")\n",
    "\n",
    "    except Exception as error:\n",
    "        log_error = dict(window=window, error=error)\n",
    "        log_errors.append(log_error)\n",
    "        pprint(log_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T01:05:39.590923Z",
     "start_time": "2021-03-30T01:05:39.434341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"pprint(log_errors)\";\n",
       "                var nbb_formatted_code = \"pprint(log_errors)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(log_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:epat] *",
   "language": "python",
   "name": "conda-env-epat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
